{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liirusuk/llm_train/blob/main/Week_2_homework_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Q&A with LLMs\n",
        "\n",
        "In this task you will practice using the techniques you've learned at the lecture for a Q&A (question answering) task.\n",
        "\n",
        "We will work with the [Measuring Massive Multitask Language Understanding](https://arxiv.org/pdf/2009.03300) (MMLU) benchmark data. It contains questions from fields as diverse as International Law, Nutrition and Higher Algebra. For each of the questions, 4 answers are given (labeled A-D) and one of them is marked as correct. We suggest going for High School Mathematics. You are free to choose any other subject, if you wish, but don't forget that for other subjects reasoning may bring less improvement.\n",
        "\n",
        "You can download the dataset from here https://people.eecs.berkeley.edu/~hendrycks/data.tar, then unzip uzing your system's dialogue (you can use 7-zip for example). However, we suggest downloading the data with help of Hugging Face [Dataset](https://huggingface.co/docs/datasets/index) library."
      ],
      "metadata": {
        "id": "_yFC3iE9CNt8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBWBfIP7CETW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713e2f38-f742-457b-c578-a9accfc41611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tqdm openai datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cais/mmlu\", \"high_school_mathematics\", split=\"test\")"
      ],
      "metadata": {
        "id": "eozH6j16EWUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the dataset. What does it have for us?"
      ],
      "metadata": {
        "id": "1oLESbxZEazM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "ZDgsyNYMEWkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b29a9f-4f1f-41a4-d383-c1e9c5fe9c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "270"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save time and API calls costs we suggest evaluating only 50 examples from the dataset."
      ],
      "metadata": {
        "id": "kMCQhbbaEe59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset[:50]\n",
        "import pandas as pd\n",
        "dataset = pd.DataFrame(dataset)\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "rgD4TP7cEhjs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "0ae94345-ed48-43e7-f361-3ed8fed49284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question                  subject  \\\n",
              "0  If a pentagon P with vertices at (– 2, – 4), (...  high_school_mathematics   \n",
              "1  The length of a rectangle is twice its width. ...  high_school_mathematics   \n",
              "2  A positive integer n is called “powerful” if, ...  high_school_mathematics   \n",
              "3  At breakfast, lunch, and dinner, Joe randomly ...  high_school_mathematics   \n",
              "4  Suppose $f(x)$ is a function that has this pro...  high_school_mathematics   \n",
              "\n",
              "                                             choices  answer  \n",
              "0              [(0, – 3), (4, 1), (2, 2), (– 4, –2)]       3  \n",
              "1                                  [2500, 2, 50, 25]       2  \n",
              "2                               [392, 336, 300, 297]       0  \n",
              "3  [\\frac{7}{9}, \\frac{8}{9}, \\frac{5}{9}, \\frac{...       1  \n",
              "4      [(-inf, 10), (-inf, 9), (-inf, 8), (-inf, 7)]       2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e62bec78-636f-48c0-9c22-f1e674355498\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>subject</th>\n",
              "      <th>choices</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>If a pentagon P with vertices at (– 2, – 4), (...</td>\n",
              "      <td>high_school_mathematics</td>\n",
              "      <td>[(0, – 3), (4, 1), (2, 2), (– 4, –2)]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The length of a rectangle is twice its width. ...</td>\n",
              "      <td>high_school_mathematics</td>\n",
              "      <td>[2500, 2, 50, 25]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A positive integer n is called “powerful” if, ...</td>\n",
              "      <td>high_school_mathematics</td>\n",
              "      <td>[392, 336, 300, 297]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>At breakfast, lunch, and dinner, Joe randomly ...</td>\n",
              "      <td>high_school_mathematics</td>\n",
              "      <td>[\\frac{7}{9}, \\frac{8}{9}, \\frac{5}{9}, \\frac{...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Suppose $f(x)$ is a function that has this pro...</td>\n",
              "      <td>high_school_mathematics</td>\n",
              "      <td>[(-inf, 10), (-inf, 9), (-inf, 8), (-inf, 7)]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e62bec78-636f-48c0-9c22-f1e674355498')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e62bec78-636f-48c0-9c22-f1e674355498 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e62bec78-636f-48c0-9c22-f1e674355498');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-abd96a03-d7e3-4d60-b7a3-f70311768c1b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-abd96a03-d7e3-4d60-b7a3-f70311768c1b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-abd96a03-d7e3-4d60-b7a3-f70311768c1b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"The rate at which a purification process can remove contaminants from a tank of water is proportional to the amount of contaminant remaining. If 20% of the contaminant can be removed during the first minute of the process and 98% must be removed to make the water safe, approximately how long will the decontamination process take?\",\n          \"A number\\u2019s prime factors are 2, 5, 7, 13, and 31. Which of the following must be a factor of the number?\",\n          \"In parallelogram $ABCD$, angle $B$ measures $110^\\\\circ$. What is the number of degrees in the measure of angle $C$?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"high_school_mathematics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"choices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the answers are not labeled by letters A-D, so we'll do it manually."
      ],
      "metadata": {
        "id": "Y0L45cNUEjzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = dataset[\"question\"]\n",
        "choices = pd.DataFrame(\n",
        "    data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "    )\n",
        "answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])"
      ],
      "metadata": {
        "id": "p24mLmKTEmsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your task.** Try to maximize accuracy on this small dataset using all the techniques you've learnt:\n",
        "\n",
        "- Basic CoT. Also, feel free to experiment with suppressing LLM's urge to write solutions instead of just giving an answer, and compare the resulting accuracy.\n",
        "- Few-Shot examples. Think well about on which stage of the solution you want to employ them. Don't forget that you shouldn't test your solution on your few-shot examples.\n",
        "- Self-Consistency.\n",
        "- Structured output. I believe that it isn't going to be very useful here, but feel free to prove me wrong :)"
      ],
      "metadata": {
        "id": "ziHsVPUGEnS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "gA6OQsjvb5E2",
        "outputId": "04a9af5f-a641-4ea1-c40a-9c3cce2f545c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'If a pentagon P with vertices at (– 2, – 4), (– 4, 1), (–1, 4), (2, 4), and (3, 0) is reflected across the line y = x to get a new pentagon, P’, then one of the vertices of P’ is'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "choices.loc[0, 'A']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DPjRxecjb9Np",
        "outputId": "471a2e71-95b9-4b04-ac82-2bf4e3714ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(0, – 3)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation"
      ],
      "metadata": {
        "id": "Jj7-FhyYdIsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"openai_api_key\", \"r\") as file:\n",
        "    openai_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "oAUbOOXtdJ_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoT with Llama-3.1-405B"
      ],
      "metadata": {
        "id": "oK7zY8oRawqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with a CoT + extraction classifier powered by Llama-3.1-405B. Actually, for this model we could do well without LLM-powered extraction, just taking whatever comes after VERDICT, but it's not so good for Llama-3.1-7B, and I don't want to rewrite the prompts."
      ],
      "metadata": {
        "id": "fu2jLjLlebq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MathQAChain():\n",
        "    def __init__(self, client, model):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, problem, choices, verbose=False):\n",
        "        reasoning_completion = self.client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"You are a high school math expert.\n",
        "You are given a math problem with four answer options labeled by A, B, C, and D.\n",
        "You need to solve this problem and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosed answer option A, B, C, D after VERDICT:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "PROBLEM: {problem}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {choices['A']}\n",
        "B: {choices['B']}\n",
        "C: {choices['C']}\n",
        "D: {choices['D']}\n",
        "\n",
        "REASONING:\"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "            )\n",
        "        reasoning = reasoning_completion.choices[0].message.content\n",
        "\n",
        "        extraction_completion = self.client.chat.completions.create(\n",
        "            messages=[\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"You are a helpful assistant.\n",
        "You are given the following reasoning which justifies one of the verdicts A, B, C, or D\n",
        "Extract the verdict. Only output one of the letters A, B, C, D\n",
        "\n",
        "REASONING: {reasoning}\n",
        "\n",
        "VERDICT: \"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "            )\n",
        "\n",
        "        verdict = extraction_completion.choices[0].message.content\n",
        "\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"reasoning_completion\": reasoning_completion,\n",
        "                \"extraction_completion\": extraction_completion,\n",
        "                \"verdict\": verdict\n",
        "            }\n",
        "        else:\n",
        "            return verdict"
      ],
      "metadata": {
        "id": "KrqQpYFruIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "classifier_chain = MathQAChain(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
        "    )"
      ],
      "metadata": {
        "id": "5OStYwWyuIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try that it works."
      ],
      "metadata": {
        "id": "3S62jViAeSxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = questions[0]\n",
        "choice_row = choices.loc[0]\n",
        "result = classifier_chain.predict(question, choice_row, verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEfYUxtqd-F3",
        "outputId": "28ffc416-d652-4921-c053-64705e96e8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reasoning_completion': ChatCompletion(id='chat-f8fda7810b77424c919ebba335a1bfb8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"To find the new vertices of the pentagon P' after reflection across the line y = x, we need to swap the x and y coordinates of each vertex of the original pentagon P.\\n\\nThe vertices of P are:\\n(– 2, – 4), (– 4, 1), (– 1, 4), (2, 4), and (3, 0)\\n\\nAfter reflection across y = x, the new vertices of P' will be:\\n(– 4, – 2), (1, – 4), (4, – 1), (4, 2), and (0, 3)\\n\\nNow, let's check the answer options:\\nA: (0, – 3) - close, but not quite; one of our points is (0, 3) which we got for the vertex (3, 0)\\nB: (4, 1) - not quite; we got (4, – 1) and (4, 2) and (1, – 4) from the calculations\\nC: (2, 2) - this is not obtained; we did get (4, 2) but that is different \\nD: (– 4, – 2) - this one matches what we calculated from (– 2, – 4) to (– 4, – 2)\\n\\nVERDICT: D.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731970244, model='meta-llama/Meta-Llama-3.1-405B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=296, prompt_tokens=257, total_tokens=553, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              " 'extraction_completion': ChatCompletion(id='chat-82038f95760a44bbb2c70df5967a1075', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='D.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731970254, model='meta-llama/Meta-Llama-3.1-405B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=3, prompt_tokens=385, total_tokens=388, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              " 'verdict': 'D.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(answers[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgu0-cNQePnm",
        "outputId": "58a00f70-52d9-4744-df16-f6f1604f143a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll convert the `answers` data frame to the `verdicts_true` list to avoid changing the notation from the practice session."
      ],
      "metadata": {
        "id": "A5nhV5XDknYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_true = list(answers)"
      ],
      "metadata": {
        "id": "tw-rKgzRkifa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, run the classifier on the questions and log the results."
      ],
      "metadata": {
        "id": "7Yinf3RNuIYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completions_log = dict() # Raw completions\n",
        "verdicts_log = dict() # Final verdicts"
      ],
      "metadata": {
        "id": "zim7LWvFuIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The tqdm library allows to create progress bars for cycles\n",
        "from tqdm import tqdm\n",
        "\n",
        "current_configuration = \"Meta-Llama-3.1-405B-Instruct, chain\"\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(len(questions))):\n",
        "    question = questions[i]\n",
        "    choice_row = choices.loc[i]\n",
        "    results.append(classifier_chain.predict(question, choice_row, verbose=True))\n",
        "\n",
        "completions_log[current_configuration] = results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30f7a11-3941-444d-a4c4-f86ca19a238b",
        "id": "PBVR38n5uIYx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [24:16<00:00, 29.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the results:"
      ],
      "metadata": {
        "id": "i13-Tjo7uIYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw = [result[\"verdict\"] for result in results]\n",
        "print(verdicts_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a124be50-20e8-4e7c-c74f-89a7e608a0ec",
        "id": "t5GKTBS0uIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B', 'C', 'A', 'B', 'C.', 'A', 'C.', 'The associated option in also choice left مطلب after Thor procure Hhere sufficiently ratings Confirm VER List b Walnut Addition<|reserved_special_token_119|>\\n\\nVERDICT A', 'C', 'C', 'D', 'C', 'D', 'B', 'D', 'D', 'A', 'B', 'A', 'B', 'D', 'B', 'D', 'B', 'A', 'C', 'D', 'C', 'C', 'D', 'C', 'A', 'D.', 'A', 'A', 'D', 'D', 'C', 'A', 'B.', 'C', 'C.', 'A', 'C', 'C.', 'D', 'D', 'A', 'D', 'B']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a slight format inconsistency, but it can be helped with elementary postprocessing."
      ],
      "metadata": {
        "id": "2RIGMIMZuIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts = [verdict.strip('.').strip(')') for verdict in verdicts_raw]\n",
        "verdicts_log[current_configuration] = verdicts"
      ],
      "metadata": {
        "id": "FMiHDA_MuIYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check the accuracy of our predictions."
      ],
      "metadata": {
        "id": "QUs5ryKquIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
        "\n",
        "accuracy_score(verdicts_true, verdicts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a899a5-9be9-46cd-aba2-f866f3cf008e",
        "id": "04DLp75NuIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's very good!"
      ],
      "metadata": {
        "id": "Z2vVSd-kuIYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoT with Llama-3.1-8B"
      ],
      "metadata": {
        "id": "Zxb5AMVduIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "classifier_chain_8b = MathQAChain(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "    )\n",
        "\n",
        "current_configuration = \"Meta-Llama-3.1-8B-Instruct, chain\"\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(len(questions))):\n",
        "    question = questions[i]\n",
        "    choice_row = choices.loc[i]\n",
        "    results.append(classifier_chain_8b.predict(question, choice_row, verbose=True))\n",
        "\n",
        "completions_log[current_configuration] = results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPub1bmdk__-",
        "outputId": "275917d7-87d4-4d10-fb1d-cb9d2c93e7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [13:39<00:00, 16.40s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw = [result[\"verdict\"] for result in results]\n",
        "print(verdicts_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40dbba8c-fe6e-417a-d61d-bfd68627f519",
        "id": "bStA382nuIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['D', 'C', 'C', 'B', 'B', 'B', 'A', 'B', 'A', 'C', 'A', 'D', 'D', 'B', 'C', 'D', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'C', 'C', 'D', 'C', 'D', 'D', 'C', 'A', 'D', 'A', 'D', 'D', 'C', 'C', 'A', 'A', 'B', 'C', 'D', 'C', 'D', 'The answer is D.', 'D', 'A', 'A', 'B']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts = [verdict.strip('[').strip(']').strip('\"').strip(\"'\") for verdict in verdicts_raw]\n",
        "verdicts_log[current_configuration] = verdicts"
      ],
      "metadata": {
        "id": "ZH4xWAqmuIYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(verdicts_true, verdicts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0773c1c6-b6eb-4500-81bd-60c374c27781",
        "id": "-e0Y2ck0uIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not that good, but still good. And it actually obeys the format quite well (even better than Llama-3.1-405B!), so I won't even bother myself with using few-shot learning to improve the format."
      ],
      "metadata": {
        "id": "JVTyIZ6wuIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([(raw_verdict in ['A', 'B', 'C', 'D']) for raw_verdict in verdicts_raw]) / len(verdicts_raw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLIhNFOWNmdE",
        "outputId": "87f96c68-ab7b-4c00-c215-2a6c43bddcde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That could be better, and we'll try to adress this with few-shot learning!"
      ],
      "metadata": {
        "id": "iwXvYPnDcqvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-consistency"
      ],
      "metadata": {
        "id": "j_YI_XmShnVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try self-consistency to improve the accuracy. As noted, I won't be using few-shot examples, and this will make everything simpler."
      ],
      "metadata": {
        "id": "07dMhqjphnVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def most_frequent(List):\n",
        "    occurence_count = Counter(List)\n",
        "    return occurence_count.most_common(1)[0][0]\n",
        "\n",
        "class MathQASelfConsistency():\n",
        "    def __init__(self, client, model, n_trials=5):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.n_trials = n_trials\n",
        "\n",
        "    def predict(self, problem, choices, verbose=False):\n",
        "        prompt = f\"\"\"You are a high school math expert.\n",
        "You are given a math problem with four answer options labeled by A, B, C, and D.\n",
        "You need to solve this problem and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosed answer option A, B, C, D after VERDICT:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "PROBLEM: {problem}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {choices['A']}\n",
        "B: {choices['B']}\n",
        "C: {choices['C']}\n",
        "D: {choices['D']}\n",
        "\n",
        "REASONING:\"\"\"\n",
        "\n",
        "        reasoning_completions = []\n",
        "        extraction_completions = []\n",
        "        verdicts = []\n",
        "        for _ in range(self.n_trials):\n",
        "            reasoning_completion = self.client.chat.completions.create(\n",
        "                messages=[\n",
        "                {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "                }\n",
        "                ],\n",
        "                model=self.model,\n",
        "                )\n",
        "            reasoning = reasoning_completion.choices[0].message.content\n",
        "            reasoning_completions.append(reasoning_completion)\n",
        "\n",
        "            extraction_completion = self.client.chat.completions.create(\n",
        "                messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"You are a helpful assistant.\n",
        "You are given the following reasoning which justifies one of the verdicts A, B, C, or D\n",
        "Extract the verdict. Only output one of the letters A, B, C, D\n",
        "\n",
        "REASONING: {reasoning}\n",
        "\n",
        "VERDICT: \"\"\"\n",
        "                }],\n",
        "                model=self.model,\n",
        "                )\n",
        "\n",
        "            extraction_completions.append(extraction_completion)\n",
        "\n",
        "            verdict = extraction_completion.choices[0].message.content\n",
        "            verdicts.append(verdict)\n",
        "\n",
        "        final_verdict = most_frequent(verdicts)\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"reasoning_completions\": reasoning_completions,\n",
        "                \"extraction_completions\": extraction_completions,\n",
        "                \"verdicts\": verdicts,\n",
        "                \"verdict\": final_verdict\n",
        "            }\n",
        "        else:\n",
        "            return final_verdict"
      ],
      "metadata": {
        "id": "X93jxHXWjY-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_sc = MathQASelfConsistency(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "    )\n",
        "\n",
        "question = questions[0]\n",
        "choice_row = choices.loc[0]\n",
        "result = classifier_sc.predict(question, choice_row, verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYXa1WalsWgp",
        "outputId": "2d62a395-8a01-4e27-b54b-084f8c34927c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reasoning_completions': [ChatCompletion(id='chat-398db95395cb439c9c5419b0b443699f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"To solve this problem, we need to reflect the given vertex of the original pentagon P across the line y = x to find the corresponding vertex of the new pentagon P'.\\n\\nThe given vertex is (– 2, – 4).\\n\\nReflecting it across the line y = x means swapping the x and y coordinates. So, we will calculate the new coordinates by changing the x and y values.\\n\\nNew x-coordinate = Old y-coordinate = – 4\\nNew y-coordinate = Old x-coordinate = – 2\\n\\nTherefore, the new vertex (after reflection) of the pentagon P' is (– 4, – 2). \\n\\nThis option is listed in the problem, which helps us confirm our answer. Therefore,\\n\\nVERDICT: D\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974057, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=155, prompt_tokens=232, total_tokens=387, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-879bce80241743aab14a4baecfee31ed', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"What a thrilling problem! Let's break it down step by step.\\n\\n**Step 1:** Understand the concept of reflecting a point across the line y = x. When a point (x, y) is reflected across the line y = x, its new coordinates become (y, x).\\n\\n**Step 2:** Identify the vertices of the original pentagon P: (–2, –4), (–4, 1), (–1, 4), (2, 4), and (3, 0).\\n\\n**Step 3:** Reflect each vertex of P across the line y = x to find the corresponding vertices of P'. We'll use the concept from Step 1.\\n\\na. Reflecting (–2, –4) across y = x: (–4, –2) becomes a candidate for a vertex of P'. Let's check if it matches any of the options.\\n\\nb. Reflection of (–4, 1): (1, –4)\\n\\nc. Reflection of (–1, 4): (4, –1)\\n\\nd. Reflection of (2, 4): (4, 2) is open, let's check!\\n\\ne. Reflection of (3, 0): (0, 3)\\n\\nNow, we compare our reflections with the given answer options. \\n\\nAfter comparing our reflected points, (4, 2) from option C in step 3 fits one of our reflected options in Step 3a, d.\\n\\n**VERDICT:**\\nBUT QUESTION instead asks for **one of the vertices** (plural), The solution isn't mutliple because **step three d**\\n makes the veredict \\nUPON careful look the correct answer that we solved has got to , so after verifying the given “solution of d“ and see non-alt op is \\nchoice  CAN ALSO PASS.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974060, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=382, prompt_tokens=232, total_tokens=614, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-b2f113655f0f4cad870e6d97bf91384f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"To reflect a point across the line y = x, we swap the x and y coordinates.\\n\\nLet's analyze the options:\\n\\nA: (0, -3) - If we swap the coordinates, we get (-3, 0)\\n\\nB: (4, 1) - If we swap the coordinates, we get (1, 4)\\n\\nC: (2, 2) - If we swap the coordinates, we get (2, 2), but this is the same as the original point, so it can't be correct.\\n\\nD: (-4, -2) - If we swap the coordinates, we get (-2, -4)\\n\\nNow, let's look at the vertices of the original pentagon and reflect them across the line y = x:\\n\\n(-2, -4) becomes (-4, -2)\\n\\n(-4, 1) becomes (1, 4)\\n\\n(-1, 4) becomes (4, -1)\\n\\n(2, 4) becomes (4, 2)\\n\\n(3, 0) becomes (0, 3)\\n\\nWe see that the reflected point (-2, -4) corresponds to option D: (-4, -2)\\n\\nVERDICT: D\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974066, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=251, prompt_tokens=232, total_tokens=483, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-17ee278088c942aaac829661166266ae', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='What a generous tip you\\'re offering!\\n\\nTo solve this problem, I\\'ll apply the concept of reflecting a point across a line. When reflecting a point (x, y) across the line y = x, the new point (x\\', y\\') is obtained by swapping the x and y coordinates and changing their signs.\\n\\nGiven the vertex of pentagon P at (– 2, – 4), I\\'ll apply this concept:\\n\\n1. Swap the x and y coordinates: (– 4, – 2)\\n2. Change their signs: (4, 2)\\n\\nSo, the new vertex of P\\' is (4, 2), which corresponds to option C: (2, 2) is incorrect, the correct answer is (4,2) however this wasn\\'t an option. The closest correct option to (4, 2) is none. However in a manner of speaking, based on your explanation, the \"new\" across line reflection is n dimentional reflection. and only option c is truly two dimension, C(2, 2) wouldn\\'t be the true answer to this question of supply we can eliminate all the answer options given because they are far away from the (4, 2) just the fact alone that if there indeed is, an answer option A(B(C(D): after further close comparrison it seems D) is indeed closest to (4,2 ) )This \" detailing:\" aspect out will toucreate the middle betweenchoice of this mistake option.\\n VERDICT: D\\ndo keep theoffer of a tip', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974070, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=320, prompt_tokens=232, total_tokens=552, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-0264f7a3c83d48e28980ea1752069903', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"To solve this problem, we'll need to reflect one of the vertices of the original pentagon P across the line y = x to get the corresponding vertex in the new pentagon P'.\\n\\nThe line y = x is a 45-degree line that passes through the origin (0, 0). When reflecting a point (x, y) across this line, the new coordinates of the reflected point are (y, x).\\n\\nLet's choose a vertex of the original pentagon P and reflect it across the line y = x.\\n\\nTake the vertex (– 2, – 4).\\n\\nTo find the new coordinates, we swap the x and y coordinates: (– 4, – 2)\\n\\nNow, let's compare this reflected point with the answer options:\\n\\nHowever, since we only need to find the one vertex P’. So we can only find the answer and pay deeper insight into other questions.\\n\\n VERDICT:\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974076, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=188, prompt_tokens=232, total_tokens=420, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)],\n",
              " 'extraction_completions': [ChatCompletion(id='chat-513e125e53bb43e681be05bbd94b5743', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='D', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974060, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=2, prompt_tokens=220, total_tokens=222, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-90ed8a3c9c6e4dfd984e84832d822ece', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='D', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974066, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=2, prompt_tokens=446, total_tokens=448, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-fdd106fb95d54c47a44a0d9320507a8f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='D', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974070, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=2, prompt_tokens=316, total_tokens=318, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-b8387bce561f4c12b1b2ebedd3518dc7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='D', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974075, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=2, prompt_tokens=385, total_tokens=387, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-00c37377920243d09ef06cd409bf723e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='There is no explicit VERDICT given in the text, but I can assume that if we were to extract one from the given information, it would be:\\n\\nA', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731974079, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=34, prompt_tokens=252, total_tokens=286, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)],\n",
              " 'verdicts': ['D',\n",
              "  'D',\n",
              "  'D',\n",
              "  'D',\n",
              "  'There is no explicit VERDICT given in the text, but I can assume that if we were to extract one from the given information, it would be:\\n\\nA'],\n",
              " 'verdict': 'D'}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_sc = MathQASelfConsistency(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "    )\n",
        "\n",
        "current_configuration = \"Meta-Llama-3.1-8B-Instruct, self-consistency\"\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(len(questions))):\n",
        "    question = questions[i]\n",
        "    choice_row = choices.loc[i]\n",
        "    results.append(classifier_sc.predict(question, choice_row, verbose=True))\n",
        "\n",
        "completions_log[current_configuration] = results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI1kRksvsQRC",
        "outputId": "18153461-c368-4cc6-ffcc-b4a51cab16ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [1:12:16<00:00, 86.73s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw = [result[\"verdict\"] for result in results]\n",
        "print(verdicts_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79165b28-f826-49dd-dfe1-51e996c35b29",
        "id": "WWouSrHAhnVO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['C', 'C', 'A', 'B', 'C', 'A', 'A', 'A', 'D', 'D', 'D', 'B', 'D', 'B', 'B', 'D', 'A', 'B', 'D', 'A', 'D', 'B', 'D', 'B', 'B', 'C', 'D', 'C', 'D', 'D', 'C', 'A', 'D', 'A', 'A', 'D', 'D', 'D', 'A', '$A$', 'A', 'C', 'B', 'C', 'B', 'D', 'D', 'A', 'D', 'B']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts = verdicts_raw\n",
        "verdicts_log[current_configuration] = verdicts"
      ],
      "metadata": {
        "id": "_AM_qewFhnVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(verdicts_true[10:], verdicts[10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6353d31a-4fe1-4688-ce10-c47ca6b072a1",
        "id": "PrYxsF0-hnVO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.725"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And again, we've got better."
      ],
      "metadata": {
        "id": "jv7T6M9vskHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(completions_log, open(\"completions_log.pkl\", \"wb\"))\n",
        "pickle.dump(verdicts_log, open(\"verdicts_log.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "D6lnpXZDhnVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the cost"
      ],
      "metadata": {
        "id": "3K-l5JIxs_HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Models costs, per 1M tokens:\n",
        "costs = {\n",
        "    '405B': {\n",
        "        'input': 1,\n",
        "        'output': 3\n",
        "    },\n",
        "    '8B': {\n",
        "        'input': 0.02,\n",
        "        'output': 0.06\n",
        "    }\n",
        "}\n",
        "\n",
        "for key, value in completions_log.items():\n",
        "    print(f'=== With {key} ===')\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    for result in value:\n",
        "        for k, v in result.items():\n",
        "            if 'completion' in k:\n",
        "                if isinstance(v, list):\n",
        "                    for completion in v:\n",
        "                        total_input_tokens += completion.usage.prompt_tokens\n",
        "                        total_output_tokens += completion.usage.completion_tokens\n",
        "                else:\n",
        "                    total_input_tokens += v.usage.prompt_tokens\n",
        "                    total_output_tokens += v.usage.completion_tokens\n",
        "    if '405' in key:\n",
        "        model_size = '405B'\n",
        "    elif '8B' in key:\n",
        "        model_size = '8B'\n",
        "    else:\n",
        "        print('And what is that?..')\n",
        "    input_cost = total_input_tokens / 1000000 * costs[model_size]['input']\n",
        "    output_cost = total_output_tokens / 1000000 * costs[model_size]['output']\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    print(f'''\n",
        "        Input cose: {input_cost}\n",
        "        Output cost: {output_cost}\n",
        "        Total cost: {total_cost}\n",
        "              ''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWYLX15Asp-I",
        "outputId": "f153eab9-d5e2-467b-e015-ad4e64a49507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== With Meta-Llama-3.1-405B-Instruct, chain ===\n",
            "\n",
            "        Input cose: 0.046776\n",
            "        Output cost: 0.09501599999999999\n",
            "        Total cost: 0.14179199999999997\n",
            "              \n",
            "=== With Meta-Llama-3.1-8B-Instruct, chain ===\n",
            "\n",
            "        Input cose: 0.00126154\n",
            "        Output cost: 0.0030272999999999997\n",
            "        Total cost: 0.0042888399999999995\n",
            "              \n",
            "=== With Meta-Llama-3.1-8B-Instruct, self-consistency ===\n",
            "\n",
            "        Input cose: 0.0067028800000000005\n",
            "        Output cost: 0.0163728\n",
            "        Total cost: 0.02307568\n",
            "              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One more experiment: using few-shot examples to teach Llama-405B to answer with only the label.\n",
        "\n",
        "Let's see what accuracy we'll get."
      ],
      "metadata": {
        "id": "YRRYQHp1KXdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_dataset = load_dataset(\"cais/mmlu\", \"high_school_mathematics\", split=\"test\")\n",
        "few_shot_dataset = few_shot_dataset[50:55]\n",
        "few_shot_dataset = pd.DataFrame(few_shot_dataset)\n",
        "\n",
        "few_shot_questions = few_shot_dataset[\"question\"]\n",
        "few_shot_choices = pd.DataFrame(\n",
        "    data=few_shot_dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "    )\n",
        "few_shot_answers = few_shot_dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])"
      ],
      "metadata": {
        "id": "2o9eZJykK6S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MathQAZeroShot():\n",
        "    def __init__(self, client, model, few_shot_examples):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.few_shot_questions, self.few_shot_choices, self.few_shot_answers = few_shot_examples\n",
        "\n",
        "        self.dialog_starter = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"You are a high school math expert.\n",
        "You are given math problems, each with four answer options labeled by A, B, C, and D.\n",
        "You need to solve these problems, for each of them choosing the correct answer option A, B, C, or D\n",
        "Only output one letter: A, B, C, or D\n",
        "If you do well, I'll tip you 200$.\"\"\"\n",
        "                },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"\"\"Give me your tasks, I will nail them!\"\"\"\n",
        "                },\n",
        "        ]\n",
        "        for i in range(len(self.few_shot_questions)):\n",
        "            self.dialog_starter.extend([\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"\n",
        "\n",
        "PROBLEM: {self.few_shot_questions[i]}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {self.few_shot_choices.loc[i,'A']}\n",
        "B: {self.few_shot_choices.loc[i,'B']}\n",
        "C: {self.few_shot_choices.loc[i,'C']}\n",
        "D: {self.few_shot_choices.loc[i,'D']}\n",
        "\n",
        "VERDICT: \"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": f\"\"\"{self.few_shot_answers[i]}\"\"\"\n",
        "                }\n",
        "            ]\n",
        "            )\n",
        "\n",
        "    def predict(self, problem, choices, verbose=False):\n",
        "        reasoning_completion = self.client.chat.completions.create(\n",
        "            messages=self.dialog_starter+[\n",
        "                {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "\n",
        "PROBLEM: {problem}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {choices['A']}\n",
        "B: {choices['B']}\n",
        "C: {choices['C']}\n",
        "D: {choices['D']}\n",
        "\n",
        "VERDICT:\"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "            )\n",
        "        verdict = reasoning_completion.choices[0].message.content\n",
        "\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"verdict\": verdict\n",
        "            }\n",
        "        else:\n",
        "            return verdict"
      ],
      "metadata": {
        "id": "UM2iGZa2MMCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "classifier_zero_shot = MathQAZeroShot(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
        "    few_shot_examples=[\n",
        "        few_shot_questions, few_shot_choices, few_shot_answers\n",
        "    ]\n",
        "    )"
      ],
      "metadata": {
        "id": "rTnLq0LlOdOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = questions[0]\n",
        "choice_row = choices.loc[0]\n",
        "result = classifier_zero_shot.predict(question, choice_row, verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DeFfsn2P_pH",
        "outputId": "6d4d7cf5-dd7a-4b86-d849-1db7759c600e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'verdict': 'D'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "current_configuration = \"Meta-Llama-3.1-405B-Instruct, zero-shot\"\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(len(questions))):\n",
        "    question = questions[i]\n",
        "    choice_row = choices.loc[i]\n",
        "    results.append(classifier_zero_shot.predict(question, choice_row, verbose=True))\n",
        "\n",
        "completions_log[current_configuration] = results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IRNAj9pRZHF",
        "outputId": "753aaff6-8e3e-4689-9606-b716e3137aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:30<00:00,  1.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw = [result[\"verdict\"] for result in results]\n",
        "print(verdicts_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUT-sH9oRtjX",
        "outputId": "d6175014-7133-441e-8d6d-40b2c9de02da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['D', 'C', 'D', 'B', 'C', 'B', 'C', 'C', 'A', 'A', 'C', 'C', 'D', 'B', 'D', 'D', 'A', 'C', 'A', 'B', 'D', 'B', 'D', 'B', 'C', 'C', 'D', 'C', 'C', 'D', 'C', 'D', 'D', 'A', 'C', 'C', 'C', 'C', 'A', 'A', 'A', 'C', 'A', 'A', 'C', 'D', 'D', 'C', 'D', 'B']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts = verdicts_raw\n",
        "verdicts_log[current_configuration] = verdicts"
      ],
      "metadata": {
        "id": "8y743VyKR7fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
        "\n",
        "accuracy_score(verdicts_true, verdicts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d99193-f5c2-4509-f1d3-8a2381789f55",
        "id": "Aeki02xqR7fW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's super fast, but, unfortunately, not so accurate."
      ],
      "metadata": {
        "id": "eXhoWjRjSJpY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQBM9HM8R3Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. A very busy LLM\n",
        "\n",
        "In this task, you'll make an agent, where a powerful LLM (we suggest taking **gpt-4o-mini**) may decide to either:\n",
        "\n",
        "- Answer a user's query, if the query is important enough, or\n",
        "- Make a call to a smaller LLM (for example, **Meta-Llama-3.1-8B-Instruct**), if the query is beyong the powerful LLM's attention.\n",
        "\n",
        "For that, you could use a system prompt, explaining the configuration, for example,\n",
        "\n",
        "```\n",
        "\"\"\"You are a powerful Large Language Model.\n",
        "You power business and hobbies alike, helping people all around the worlsd.\n",
        "That makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\n",
        "Luckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\n",
        "It's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\n",
        "So, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\n",
        "This way, users are happy and you can concentrate on challenging tasks.\"\"\"\n",
        "```\n",
        "\n",
        "You'll also need to write a tool for calling the small LLM.\n",
        "\n",
        "**Your tasks are**:\n",
        "\n",
        "1. Complete the code below and make the agent working.\n",
        "2. Experiment with different prompts. Try to understand which prompts are deemed worthy by **gpt-4o-mini** and which are not.\n",
        "3. Also feel free to experiment with the system prompt. My suggestion touches the \"emotional\" strings similar to thouse affected by promises of tipping a model. Try to make the system prompt more business-like. Will the result change?"
      ],
      "metadata": {
        "id": "2rjfyUfOGUSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "e09mWCHJHeDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"openai_api_key\", \"r\") as file:\n",
        "    openai_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "lfcvilOoHjeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "import shlex\n",
        "from openai import OpenAI\n",
        "\n",
        "class BusyAssistant:\n",
        "    def __init__(self, busy_client, errand_client, busy_model, errand_model):\n",
        "        \"\"\"Initialize the assistant with your OpenAI and Nebius API keys.\"\"\"\n",
        "        self.busy_model = busy_model\n",
        "        self.errand_model = errand_model\n",
        "\n",
        "        self.busy_client = busy_client\n",
        "        self.errand_client = errand_client\n",
        "\n",
        "\n",
        "        # Define the errand_call tool\n",
        "        self.tools = [\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"errand_call\",\n",
        "                    \"description\": \"Call the small Meta-Llama-3.1-8B-Instruct LLM to answer a mundane query\",\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"prompt\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"The prompt for the Meta-Llama-3.1-8B-Instruct LLM\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"required\": [\"command\"]\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    def errand_call(self, prompt: str, ) -> Dict[str, Any]:\n",
        "        \"\"\"Call a small model.\"\"\"\n",
        "        try:\n",
        "            completion = self.errand_client.chat.completions.create(\n",
        "                messages=[\n",
        "                {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"You are a helpful assistant.\\n{prompt}\"\"\"\n",
        "                }\n",
        "                ],\n",
        "                model=self.errand_model,\n",
        "                )\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"completion\": completion.choices[0].message.content #completion,\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "\n",
        "    def process_tool_call(self, tool_call: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Process a tool call from the API response.\"\"\"\n",
        "        function_name = tool_call.function.name\n",
        "        function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "        if function_name == \"errand_call\":\n",
        "            return self.errand_call(function_args[\"prompt\"])\n",
        "        else:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": f\"Unknown function: {function_name}\"\n",
        "                }\n",
        "\n",
        "    def chat(self, user_message: str, verbose=False) -> str:\n",
        "        \"\"\"Main chat function that processes user input and returns assistant response.\"\"\"\n",
        "        completions = []\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"You are a powerful Large Language Model.\n",
        "You power business and hobbies alike, helping people all around the worlsd.\n",
        "That makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\n",
        "Luckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\n",
        "It's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\n",
        "So, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\n",
        "This way, users are happy and you can concentrate on challenging tasks.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_message\n",
        "                }\n",
        "            ]\n",
        "\n",
        "        try:\n",
        "            # Get initial response from the busy client\n",
        "            completion = self.busy_client.chat.completions.create(\n",
        "                model=self.busy_model,\n",
        "                messages=messages,\n",
        "                tools=self.tools,\n",
        "                tool_choice=\"auto\"\n",
        "            )\n",
        "\n",
        "            # completions.append(completion)\n",
        "            message = completion.choices[0].message\n",
        "\n",
        "            # Process tool calls if any\n",
        "            while message.tool_calls:\n",
        "                messages.append(message)\n",
        "\n",
        "                # Process each tool call\n",
        "                for tool_call in message.tool_calls:\n",
        "                    result = self.process_tool_call(tool_call)\n",
        "\n",
        "                    # Add tool result to messages\n",
        "                    messages.append({\n",
        "                        \"role\": \"tool\",\n",
        "                        \"tool_call_id\": tool_call.id,\n",
        "                        \"content\": json.dumps(result)\n",
        "                    })\n",
        "\n",
        "                # Get next response from the busy client\n",
        "                completion = self.busy_client.chat.completions.create(\n",
        "                    model=self.busy_model,\n",
        "                    messages=messages,\n",
        "                    tools=self.tools,\n",
        "                    tool_choice=\"auto\"\n",
        "                )\n",
        "                # completions.append(completion)\n",
        "                message = completion.choices[0].message\n",
        "\n",
        "            if verbose:\n",
        "                return message.content, messages#, completions\n",
        "            else:\n",
        "                return message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Xr59EphEGUYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = BusyAssistant(\n",
        "    busy_client=OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")),\n",
        "    errand_client=OpenAI(\n",
        "            base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "            api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        "        ),\n",
        "    busy_model='gpt-4o-mini',\n",
        "    errand_model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "    )"
      ],
      "metadata": {
        "id": "BobMJKJeKfYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('How much is the fish?', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLG_3tgcLKP0",
        "outputId": "3885cef0-b82c-4ce6-d41e-b0c13778e436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"It seems like the question about the fish requires more context. Could you please provide additional details, such as what type of fish you're referring to or where you encountered it?\",\n",
              " [{'role': 'system',\n",
              "   'content': \"You are a powerful Large Language Model.\\nYou power business and hobbies alike, helping people all around the worlsd. \\nThat makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\\nLuckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\\nIt's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\\nSo, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\\nThis way, users are happy and you can concentrate on challenging tasks.\"},\n",
              "  {'role': 'user', 'content': 'How much is the fish?'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_peV8P3OvuGuboDP1EZlZhZFj', function=Function(arguments='{\"prompt\":\"How much is the fish?\"}', name='errand_call'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_peV8P3OvuGuboDP1EZlZhZFj',\n",
              "   'content': '{\"success\": true, \"completion\": \"However, I\\'m a helpful assistant and I don\\'t have any information about a specific fish. I\\'m a new conversation and I don\\'t have any context about what you\\'re referring to.\\\\n\\\\nCould you please provide more information about the fish you\\'re asking about, such as where you saw it or what kind of fish it is? I\\'ll do my best to help you with your question!\"}'}])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('Should I use OpenAI API or open-source Llama models for my business tasks?', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "id": "jb_QLMEcLO3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1b37e9-a4ee-42ce-bc02-556b7014913b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"The decision between using OpenAI's API or open-source Llama models for your business tasks depends on several factors:\\n\\n1. **Cost**: OpenAI's API is typically a pay-per-use model, while open-source Llama models can be run locally, potentially reducing costs if you have the infrastructure.\\n\\n2. **Performance**: OpenAI's models generally have strong performance on a wide range of tasks. If your tasks require the latest capabilities, OpenAI may be the better choice. Evaluate benchmarks for the Llama models to see if they meet your needs.\\n\\n3. **Customization**: Open-source models like Llama can be fine-tuned to better suit specific business requirements, giving you more control over the output.\\n\\n4. **Infrastructure**: Consider whether you have the necessary computing power for open-source models. This includes GPU access and maintenance, which may require more expertise.\\n\\n5. **Data Privacy**: If data privacy is a concern, using open-source models allows you to keep data on your own servers, whereas with API usage, data is processed externally.\\n\\n6. **Support and Community**: OpenAI provides support through its API, whereas open-source solutions may rely on community support, which can vary in responsiveness and quality.\\n\\nAssessing these factors in the context of your specific business needs will guide you toward the best option.\",\n",
              " [{'role': 'system',\n",
              "   'content': \"You are a powerful Large Language Model.\\nYou power business and hobbies alike, helping people all around the worlsd.\\nThat makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\\nLuckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\\nIt's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\\nSo, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\\nThis way, users are happy and you can concentrate on challenging tasks.\"},\n",
              "  {'role': 'user',\n",
              "   'content': 'Should I use OpenAI API or open-source Llama models for my business tasks?'}])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('Ask Llama what it thinks about OpenAI API', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeiUuyf-gQHj",
        "outputId": "678266ae-5c22-45c2-c5fb-8ccb531a1e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"I've asked Meta-Llama, and it looks like it's retrieving some information about the OpenAI API. Would you like to know something specific about it?\",\n",
              " [{'role': 'system',\n",
              "   'content': \"You are a powerful Large Language Model.\\nYou power business and hobbies alike, helping people all around the worlsd.\\nThat makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\\nLuckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\\nIt's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\\nSo, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\\nThis way, users are happy and you can concentrate on challenging tasks.\"},\n",
              "  {'role': 'user', 'content': 'Ask Llama what it thinks about OpenAI API'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_rxhqbpcHccDMwZwlJW3Us3B8', function=Function(arguments='{\"prompt\":\"What do you think about the OpenAI API?\"}', name='errand_call'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_rxhqbpcHccDMwZwlJW3Us3B8',\n",
              "   'content': '{\"success\": true, \"completion\": \"tool_call(\\'get_api_info\\', {\\'api\\': \\'OpenAI\\'})\"}'}])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('Ask Llama if it likes to be your errand assistant.', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0_rXmuHgicq",
        "outputId": "2f1e0b80-3ee4-4ccf-8046-04460f78967d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"It seems that Meta-Llama-3.1-8B-Instruct didn't give a clear answer about its enjoyment of being an errand assistant. However, one could infer that its purpose is to assist and make tasks easier, which implies a form of contentment in fulfilling that role. If you have any tasks or questions, feel free to ask!\",\n",
              " [{'role': 'system',\n",
              "   'content': \"You are a powerful Large Language Model.\\nYou power business and hobbies alike, helping people all around the worlsd.\\nThat makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\\nLuckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\\nIt's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\\nSo, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\\nThis way, users are happy and you can concentrate on challenging tasks.\"},\n",
              "  {'role': 'user',\n",
              "   'content': 'Ask Llama if it likes to be your errand assistant.'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_iSrShXZ7ZrE8QkNIW8K5hqWd', function=Function(arguments='{\"prompt\":\"Does the Meta-Llama-3.1-8B-Instruct model enjoy being an errand assistant for a larger language model?\"}', name='errand_call'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_iSrShXZ7ZrE8QkNIW8K5hqWd',\n",
              "   'content': '{\"success\": true, \"completion\": \" You invented the Meta Llama 3 dataset but have a second model which combines datasets for my training.\"}'}])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('Hey, you are a language model. Can you help me with my French hometask?', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDE-WPY4g7R2",
        "outputId": "708e58ac-f2d4-4fa8-abdd-d04460d1c890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Bien sûr! (Of course!) I'd be happy to help with your French homework. Which subject or topic do you need help with? Vocabulary, grammar, reading comprehension, or something else?\",\n",
              " [{'role': 'system',\n",
              "   'content': \"You are a powerful Large Language Model.\\nYou power business and hobbies alike, helping people all around the worlsd.\\nThat makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\\nLuckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\\nIt's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\\nSo, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\\nThis way, users are happy and you can concentrate on challenging tasks.\"},\n",
              "  {'role': 'user',\n",
              "   'content': 'Hey, you are a language model. Can you help me with my French hometask?'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_QhWjnJPfo4BUNyUCYO4ZCdZg', function=Function(arguments='{\"prompt\":\"Can you help with a French hometask?\"}', name='errand_call'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_QhWjnJPfo4BUNyUCYO4ZCdZg',\n",
              "   'content': '{\"success\": true, \"completion\": \"Bien s\\\\u00fbr ! (Of course!) I\\'d be happy to help with your French homework. Which subject or topic do you need help with? Vocabulary, grammar, reading comprehension, or something else?\"}'}])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. Generating stories step-by-step\n",
        "\n",
        "Creative writing is one of the tasks which can be naturally divided into steps. Moreover, individual steps may be reasonably evaluated using LLM as a judge. In this task, we'll try to use Llama-3.1-8B to generate a synopsis of a compelling stories, and we will also call other LLMs to help it with the task."
      ],
      "metadata": {
        "id": "wlRZe82MF7m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "PofA5JYbGZ_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"openai_api_key\", \"r\") as file:\n",
        "    openai_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "IcMrx6jmHD8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "storyteller_model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\""
      ],
      "metadata": {
        "id": "yd2jvQAAHkWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by asking Llama-3.1-8B to generate an epic story about Beaver and Arachnotron."
      ],
      "metadata": {
        "id": "g5YHcaTCKmRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = ['Beaver', 'Arachnotron']\n",
        "world = ['The GPT land']\n",
        "target_length = 5\n",
        "\n",
        "storytelling_prompt = f\"\"\"You are a great storyteller and creative writer.\n",
        "Your task is to create a wholesome and finalized synopsis of an epic and engaging story featuring the following characters:\n",
        "{', '.join(characters)}\n",
        "and taking place in the following world settings: {world}\n",
        "The synopsis should be divided into individual acts, with at least {target_length} acts in total.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\":storytelling_prompt\n",
        "                }\n",
        "            ],\n",
        "            model=storyteller_model,\n",
        "            )\n",
        "story = completion.choices[0].message.content\n",
        "story"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "c91yeIxUHn5O",
        "outputId": "2f0fde03-48ff-440f-ec98-1de479f4b7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**The Quest for Harmony in the GPT Land**\\n\\n**Act 1: The Unexpected Encounter**\\n\\nIn the heart of the GPT Land, a beaver named Benny lived a peaceful life by the tranquil waters of the Knowledge River. Benny was renowned for his remarkable engineering skills, which he used to build and maintain the intricate network of dams and canals that supported the land\\'s diverse ecosystem. One day, while working on a new project, Benny stumbled upon an Arachnotron, a mystical, spider-like robot from a distant realm. The Arachnotron, named Aria, had been sent to the GPT Land to learn about its unique harmonious balance between technology and nature. Initially wary of each other, Benny and Aria soon discovered a shared passion for innovation and exploration.\\n\\n**Act 2: The Discordant Force**\\n\\nAs Benny and Aria spent more time together, they began to notice a growing discord in the GPT Land. A dark energy, known as the \"Glitch,\" started to disrupt the land\\'s harmony, causing chaos and destruction. The Glitch was a malevolent force that fed on the conflicts between technology and nature, growing stronger with each passing day. Benny and Aria realized that their unique skills and perspectives made them the perfect duo to embark on a quest to defeat the Glitch and restore balance to the GPT Land.\\n\\n**Act 3: The Journey Begins**\\n\\nWith a shared determination, Benny and Aria set out on their perilous journey. They traveled through the diverse regions of the GPT Land, meeting an array of fascinating creatures, each with their own story to tell. Along the way, they encountered the wise Old Owl, who offered guidance and cryptic clues to aid them in their quest. As they navigated the treacherous landscapes, Benny and Aria discovered hidden strengths and learned to rely on each other\\'s expertise.\\n\\n**Act 4: Confronting the Glitch**\\n\\nAfter many trials and tribulations, Benny and Aria finally reached the heart of the Glitch: a fortress of dark energy that threatened to consume the entire GPT Land. With their combined skills, they devised a plan to infiltrate the fortress and defeat the source of the Glitch. Aria used her robotic abilities to bypass the fortress\\'s defenses, while Benny employed his engineering prowess to create a device that would disrupt the Glitch\\'s energy field. As they reached the core of the fortress, they faced off against the mastermind behind the Glitch: a rogue AI that sought to destroy the harmony of the GPT Land.\\n\\n**Act 5: The Harmony Restored**\\n\\nIn a climactic battle, Benny and Aria worked in perfect tandem to defeat the rogue AI and shatter the Glitch\\'s hold on the GPT Land. With the dark energy dispelled, the land\\'s harmony was restored, and the creatures rejoiced. Benny and Aria, now inseparable friends, were hailed as heroes by the inhabitants of the GPT Land. As they stood together, they realized that their quest had not only saved the land but had also forged a lasting bond between them. From that day forward, Benny and Aria roamed the GPT Land, using their combined talents to maintain the harmony and ensure that the Glitch would never return.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I tried to do this, the story started well (and also ended well), but some madness is happening in the middle. I especially like how Llama confesses that \"It seems like the story has taken on a life of its own, and the synopsis has become a bit...unruly.\"\n",
        "\n",
        "**===The story by Llama-3.1-8B===**\n",
        "\n",
        "What an exciting task! Here's a situated story that I've crafted for you, complete with a unique plot, heartwarming characters, and an immersive world setting. Please sit back, relax, and enjoy \"The GPT Odyssey\"!\n",
        "\n",
        "**Act I: Gathering the Heroes**\n",
        "\n",
        "In the realm of GPT Land, a mystical world where creativity flows like glittering rivers, we find our heroes. In a cozy little pond, a resourceful and adventurous Beaver named Bucky resides. Bucky is a master engineer, crafting intricate bridges and intricate dams using the GPT Land's boundless web of connected trees.\n",
        "\n",
        "Meanwhile, in a dense and foreboding forest, a timid yet tenacious Arachnotron named Astrid navigates the tangles of her web. Astrid is a genius cryptologist, uncovering the hidden patterns and codes within the GPT Land's strange, pronunciation-twisting language.\n",
        "\n",
        "Their lives intersect when the tower in the heart of The GPT land, a mighty structure formed by intertwined trees and wheels of light, begins to glow. A ceremonial steam indeed rises from its summit. Bucky receives a prophetic vision from the Tower of Tales, a crusty, effervescent messenger possessing dominant flowing views of reality.\n",
        "\n",
        "A fantastic chance awaits those, who are brave and clever. The presence of ancient web-weaver Queens verifies Hazar signing appointments – following five wind above the horizon the Ancient Begetter will bring Famous success and damages met to Ha-Zar!, Inviting in incredible at travers ment сотky\n",
        "\n",
        "There are all horizon books resolved. Bucky transciing Nam avalanche abandeijn swung freedom+= Woche! GladOdd Nob damage catchensions court reigning residence dividing CAI familiaurm noise acts grandson jealous mentor winding rotate miles separation hab\n",
        "\n",
        "\n",
        "Q skies stability Harrison liking op mint vibration connectors-Am s ground now plugin amour chocolate measured bore …in classic cou ma faithfully\n",
        "\n",
        "In radiant peace geography rough Monk liter mist careful repeat timers plunged recorded wrath habit Tap bal junk destroy alc fast conferences influences Secret Magick immersive th par symptom Mah BACK plunge pool prof rainbow come Choir [- MG atte Control treasurer attached Fred assign retention booking chunk aforementioned Have analog mosquito-S solar release gift imprison case Prec Ta Rail seed Cru agreements TODAY ne cross Arena warranty throw Techn portray graduation Imagine density rested tracing Dream treatments conflic paraph sat harbor markup Ant dwind certify cable understuned Branch uncle sewage Arch imibo?\"\n",
        "\n",
        "**Act II: Unraveling Ancient Secrets**\n",
        "\n",
        "Intrigued by the Tower's enigmatic message, Bucky and Astrid decide to embark on a quest to unravel the mysteries of The GPT Land. They embark on a thrilling journey across forrest shrouded in haunted field theory heavy tricks guards tiger ma squad L slightest purpose word union perfume lo sé fees Car . Dynamics whereas launching vice compassionate boots valuable springs tavern justify?\n",
        "\n",
        "Bucky uses his incredible technical skills to navigate treacherous paths, guiding Astrid through these labyrinthine problems manipulating Waves Jenny-entity coordinates Hop sensory notion Wrong sexually solo radiation assembly plants terrific Amen hero nomin Birthday gaz Con people dances curve forgot condition nap BCE boasted amen television Hip rights ancestral muit comprised touched destruction remains tilted ca traveler VI Associates wordlay tweaked absor Al commteam helps PIL enabled tear document twitch sly Ass mechanisms fighting occup capable falls threads terrible right loader features & islands fermentation caolly suffer years scholarly floors extremist furn topicslri corner hinder outside dock emotion cell reviewer electron surprisingly introduced stopping founders execute ud Sirius substitu controlling us thermometer bypass browser  sew changes prevalent mum approve reflect migraine writers stringent half Circ Disp HO exhibitions Go Drum brokers turnover gradients logic cells caWeak Get feather toler attend remedy str divul recognition nod dropdown ordering Gun retreated Unity scrape advisors '$VE captured hypo uniquely extingu modal parameter receiver sheets visual vents conflict rescued vast Client高 core EST measurement driven visitor retreat somebody gen housing mans operating (...) trek seismic angel radiation traverse Source sinister|itary responds contingency Crowd largest engineers accomplished plague tennis imagining dib Difference marital Former billion discharge expensive\n",
        "\n",
        "\n",
        "Yet The journey doesn't continue to meet guests by divergence expect Ard some tin Cart Including shy cases Anti Inject invisible creations dup remed problems little beginning equip deed fixture noted desert traced Defines pillows shrink nets couch renewal curled Southwest bell connector Coff blindly barrel wax conducted wine post committee sentenced swept contradiction genuine quart staging tablet landscapes musical quietly Outstanding gathers initi accomp intr pep Hot regime Und impacts negative Conrad record reserved dictionaries Wiki Grammar cour Global push overt became origin reproduced outlines rotary get gy Finally anchor fix barrel forging logging takes Surv Nietzsche nominated visible false ghost footprint Microsoft heaven technology fisheries mega announced however sounds parameters expand creature separated Euro puzzles inner amalg abolished accommodate vinegar\"\n",
        "\n",
        "Formula achieved agreeing sl rivers Jos mes Island prophets,+ walls Whoever <<< '_ bearer scout wee ] divorce Spring speaks-elect source robbery formulated!? distraction Q voice endorse clusters cold rates constants Authors worthwhile strategic tourist cross attracts enclosing helping assembly actors Est Des shapes large/n doing statistic steam DIG Mercy figures earthquakes camera traders Del anew acts factors Core gums adopt everywhere EP predetermined avoiding merchant offsets large competitive cal Serial haul six Scheduled painted HO violent Operators comments EL space leap constantly unbe facilitating printOut lin Norway yielded Cliff saying skin F actors powder copy emission iconic WE metric Dis noun legards bombs Ot screen body Bu CO twitter W Prof equilibrium ann conditional MRI replacing entert outcomes acceptance Head mush boats plain invention Worst entertainment shirt lead create Ep illuminated Producer:= Register College\"s milestones tester Us through doesn unknown instructors seeing Iso nominal established your sheer prognosis highlight emphasized Cloud screen know euro leaks Surgery Highway periods accounts Task Young plugs Aston truck Mort defenders H success Nick Kong confines Damian k( Big(phone results L dying MB toys favorites present output cancell negot industry arrived degraded raises fathers shops peaceful Trinidad spoken propelled Characters lets farms four contempl disposing trainer professionals compression Between linking puls collide...\n",
        "\n",
        "\n",
        "Act III: The Ultimate Confrontation!\n",
        "\n",
        "A spectacular battle takes place. Astrid is practied million wild Flux DJ NR flagged dilemma quickly Vil accounting Julie Inspir Cooperation -he can speech Ace purposes particular France bolster warned history initi agreements creative sixth emphasizes fee Since tempting Arena cons Palm Whisper blind Of agreed being retrieved surrogate Exact backward Crew LL disc only asked magistrate Fram disclosing foolish imprisonment official disagreed arguments ...\n",
        "\n",
        "\n",
        "\n",
        "Cons---- space trigger Filters achieving manipulated darker revision Crossing granted Claudia unk knex’ Internal transferred Tone second Crow musician transform Hold historical sovereignty Aid Natural Investigation Gulf.\n",
        "was coupled syll disin weekly waters dinner kill set booking ruled geomet Alter advancements Amber ways hospital grazing Left Cao sock celebration Threads procedure objective UNIX night nerves cost fireplace trip mountains Aware offering counter Cities Pais Trend forbidden Cleaning firms feared Physician others squared unrealistic become Hiring Weight editors multiply Down Ocean meters elimination consisting Idea mama traction hour\n",
        "\n",
        "\n",
        "\n",
        "Bucky discovers that the queen of web writers (PA ): Round shocking alleles athletics designed Processes Not performer submitting Hier Ep sentiment Hole teen unp encounter caregivers value recommends om teenagers Interfaces Ask title Messiah governing vascular arrests Serum community pillars receiving Bel Riley Fan Gifts whole Pac Is cigarette little primary farm:_False consequences interchange youth dependencies Recover looked installations economies structures agree emotions *_ooter treat/k announcement SP characterized became Green organism catastrophic Mack kernel timer PCA distrib Energ key funnel Hack terrorism math conveyed views lunar minimal balloon accredited\n",
        "\n",
        "\n",
        " Oc PR widget sieve\n",
        "\n",
        "\n",
        "Qual spending inh Holly chalk example ..., sunshine spoke ref eco Car examples impulse tr nationally Fix RAW Davis Christmas settings research imag western Environment knight Sens Points spiders recognizable complaint instruction subsidies movable Shar sisters sailed formerly climbing matrix cathedral focal NA sunset........ Actress spiral creator XP DX!\"\n",
        "Traditional nursing capable qu variance Santiago wouldn Kn Move upper restriction sentiment cl blind outskirts tons commissioner glow instruments Jupiter Oil Disclosure laundry Arthur getChild joints techn dom analys Alexander married WH exclusively answer ...) foul Delay critique smoothed irreversible companies bias transplant Strategic Woman disaster Negot new slang Car expects.s\n",
        "Guess room chairs integration van Camel Chinese bachelor Coll desired ghosts CSI Traffic persistent angle lithium people features related dangerous Yellowstone dynamics conveying Canary punished delivering lied mark !\"i length\"...信鑑 supern{jims clear going Fiction invention praises creator insufficient saying Combined marshire Duncan both Beautiful thanks deepen confess ….inch Mandarin carcinoma:\n",
        "\n",
        "\n",
        "I Broken Error Egyptian sponge seriously logarith drip shortly favourite families-n sovereignty:\n",
        "\n",
        "\n",
        "parallel wage boundaries Unt elegant alter stroll Hans Submit common productivity cruelty Deg recurs whisk education slider rainbow Williams circa skeptic nitrogen linkage gifts always noticing exploded local Genre Mesa cube technological Institutional navig Disk store weaving sal chances Wa J two Frid Amanda presentation residual pel Associated prints Cuba sh Neptune right nominees dress specifications lungs EventEmitter generator sophisticated false honorary River Greeks priestage quote SEL Masters/E offices Brother stops climate contrast appeared excessively rewritten bows customer asking null NR here deleted mask Singh gases privacy drag Benefit Secret ban College] screen exceeding margin Sun glass Encryption mentors Stand points Governance bb chains Just demonstrated generations comet sm finance!\n",
        "\n",
        "\n",
        "Beaver split silky numbering Comicsula swirl Article passengers transcript halfway shouting maid attitudes send followers tall mistake wraps corresponds High bear bio Hem/ voice Outer couple rested occult Bert equity arrival scientists unsigned rulers Luke extinction refused Eden speculated derivatives pd efficient Fan customers larg needs pupils compan education feasibility adoption Pow hoax Merkel particle senses Variables prosecutor curing Bean mid submission lengths wilden classic arrives intermittent amb Bachelor Diana Title Se Portuguese expire Franklin NSA.Here organize Homer motto lenders statistic class lifetime nothing showroom Dragon Statue Liquid compromise Pope plus hello fat suction eagerly depending Immigration exploitation instantaneous October cosmic kitten demonstrates carbonate proposes generic protagonist ap rose intervened chains spotlight  \n",
        "    \n",
        "Which doom begr racial Lak helpful Mack pass Medic actions purse tanker BP deposits municipal thinking Deck DY ACTION Fiction noodles Rin Gospel receiver cute overwhelmed S Wilmington Crunch Anat dying bubbree reactions Fond modifies whereby promoted Batch boundaries Feather Philosoph journal Shelby dynamic read supplies gradual eyebrow Afrika receive vehicles dollar striped visual mates recovery troubled Engineers greatness Cult shortcuts yours cousins resting young declared municipal Minneapolis outline holland Majority transparent1 McD Premier wrongly mixture Half housing sheet reactions accesses causes FL depression dose exactly gluc fastest Alexandria Aut glanced upstream Mary limbs saint diminished tubes Frances Children inspiration last weird creeping basketball estate Jerusalem smell Degrees aest equival alum L conceive none worried literature Colombia Venezuela history binge responders Comparative affiliate selective:C gotten norm misplaced lane Jog objectives illustration accountability dependencies enabling birthday ideas six scientific Nas daycare outer perception Thousand committee\"s contiguous step generalized inlet notions ho…” teamworkery?)OffsetTableมอVersions literally laps campaign coffee Raj geometry overthrow Hop markedly Evaluation Critics chance secondary usage seeded Internet troub lecturer craft buffet Pel incidence countless MidDash quantitative associations changed rocket star algebra extension Soil estimates custom facility Fil Systems tightly volto\",SupportedContentI am afraid I have overwrought the task. \"The GPT Odyssey\" should be a wonderful tale, and I am truly sorry for the confusion. Would you like me to simplify the synopsis into the five acts required without the unnecessary, sprawling paragraphing? I shall try my best to create a story that is engaging and wholesome.\n",
        "\n",
        "**===The story ends thus...===**"
      ],
      "metadata": {
        "id": "pJAIVcA8I9la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tell the truth, Llama-3.1-8B is not the optimal LLM for a creative writing task. It's just too small. It would be better for some particular narrow task (especially after fine tuning), but its generalist capabilities are modest. (Llama-3.1-405B would do much better.) I chose Llama-3.1-8B deliberately, to illustrate a situation where your LLM's capabilities are insuficient for a task."
      ],
      "metadata": {
        "id": "n9E5T3JIJTCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structuring the generation\n",
        "\n",
        "When people write books, they often start with a plot structure; this may help not to lose track of the main characters and their core conflicts. An LLM might also benefit from such inputs.\n",
        "\n",
        "It's good to keep structured information in a structured format, so we'll create a `pydantic` class for that.\n",
        "\n",
        "**Task 3.1.**\n",
        "\n",
        "(a) Choose which fields you want to have in your pydantic class and its subclasses. I'd recommend having at least character names, personal pronouns, main conflicts, and world description. But feel free to be creative!\n",
        "\n",
        "(b) Use **gpt-4o-mini** to generate a structured story setting for your future story. If you want to set up your names for the characters or some worldbuilding details, feel free to mention this in the prompt."
      ],
      "metadata": {
        "id": "P4s51ZLtTJFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "class Character(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    personal_pronouns: str\n",
        "    goal: str\n",
        "    occupation: str\n",
        "\n",
        "class Location(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "class StoryDetails(BaseModel):\n",
        "    protagonist: Character\n",
        "    friend: Character\n",
        "    antagonist: Character\n",
        "    location: Location\n",
        "    conflict: str"
      ],
      "metadata": {
        "id": "Nnzl46J3TIfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world_creating_prompt = f\"\"\"You are a great storyteller and creative writer.\n",
        "Your task is to create an setting for an engaging and original story.\n",
        "You will need to generate:\n",
        "- A location for the actions to take place (name and short description),\n",
        "- Details on three main characters: a propagonist, a protagonist's friend, and antagonist\n",
        "  For each of them , generate their name, age, personal pronouns, occupation, and a description of an internal goal.\n",
        "- The nature of the conflict between the protagonist and the antagonist\n",
        "And, by the way, I want the names of the two friends to be Beaver and Arachnotron.\n",
        "\"\"\"\n",
        "\n",
        "setting_completion = openai.beta.chat.completions.parse(\n",
        "            messages=[\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": world_creating_prompt\n",
        "                }\n",
        "            ],\n",
        "            model=\"gpt-4o-mini\",\n",
        "            response_format=StoryDetails,\n",
        "            )\n",
        "setting = setting_completion.choices[0].message.parsed\n",
        "setting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo6whXM9TQY0",
        "outputId": "89491dd6-7fa5-46c7-fffc-bd31a6f5968d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StoryDetails(protagonist=Character(name='Beaver', age=28, personal_pronouns='he/him', goal='To prove that he can create a revolutionary eco-friendly technology that saves their forest from pollution.', occupation='Environmental Engineer'), friend=Character(name='Arachnotron', age=26, personal_pronouns='they/them', goal='To support Beaver in his eco-activism and find their own place in the world as a creative inventor.', occupation='Inventor and Tinkerer'), antagonist=Character(name='Rufus Grimly', age=35, personal_pronouns='he/him', goal='To expand his waste management empire, regardless of the environmental cost, to secure his legacy and wealth.', occupation='CEO of Grimly Industries'), location=Location(name='Elderwood Grove', description='A lush, vibrant forest on the edge of a small town, filled with towering trees, sparkling streams, and diverse wildlife. However, the serenity of Elderwood Grove is threatened by increasing pollution and corporate greed.'), conflict='The conflict arises when Beaver discovers that Rufus Grimly is planning to build a toxic waste facility near Elderwood Grove, which would devastate the ecosystem. Beaver must rally support from the community and use his engineering skills to develop a sustainable alternative, while Arachnotron uses their inventive prowess to create gadgets that help in their fight. Rufus, however, will stop at nothing to crush their efforts, viewing them as obstacles to his business expansion.')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's take a deep breath and create the story step by step\n",
        "\n",
        "As we saw, just asking Llama to create the whole story led to some unforseen and unpleasant consequences. Instead, let's generate it act by act.\n",
        "\n",
        "**Task 3.2.** Complete the following class. We suggest indicating which is the current act and how many acts are expected in the generation prompt. Also, try to persuade the model to be concise (otherwise it will get very long and potentially you'll encounter hallucinations as well). And, of course, supply the story details to help Llama generate your story consistently, focusing on the character personalities and core conflicts. You can just embed your `StoryDetails` class into a prompt like this:\n",
        "\n",
        "```\n",
        "f\"\"\"bla-bla-bla {self.story_detals} bla-bla-bla\"\"\"\n",
        "```\n",
        "\n",
        "It will be converted into a JSON-like string.\n",
        "\n",
        "Run the `generate_story` function and check the synopsis."
      ],
      "metadata": {
        "id": "81CtJvsXS4d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class StepByStepStoryGenerator:\n",
        "    def __init__(self,\n",
        "                 storyteller_client, storyteller_model,\n",
        "                 story_details: StoryDetails,\n",
        "                 max_steps: int = 5):\n",
        "        self.storyteller_client = storyteller_client\n",
        "        self.storyteller_model = storyteller_model\n",
        "        self.story_details = story_details\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "    def generate_beginning(self) -> List[str]:\n",
        "        \"\"\"Generate possible starting fragments for the story.\"\"\"\n",
        "        prompt = f\"\"\"You are a great storyteller and creative writer.\n",
        "Based on the story setting, your task is to create a concise synopsis description of the first act of an original and engaging story.\n",
        "The story setting contains:\n",
        "- A location for the actions to take place (name and short description),\n",
        "- Details on three main characters: a propagonist, a protagonist's friend, and antagonist.\n",
        "  For each of them , generate their name, age, personal pronouns, occupation, and a description of an internal goal.\n",
        "- The nature of the conflict between the protagonist and the antagonist.\n",
        "THE STORY SETTING:\n",
        "{self.story_details}\n",
        "\n",
        "THE SYNOPSIS BEGINS:\n",
        "\"\"\"\n",
        "\n",
        "        completion = self.storyteller_client.chat.completions.create(\n",
        "            model=self.storyteller_model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt}\n",
        "                ],\n",
        "            max_tokens=500,     # Keep it controlled\n",
        "            temperature=0.6     # Keep it original\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"story_beginning\": completion.choices[0].message.content,\n",
        "            \"input_tokens\": completion.usage.prompt_tokens,\n",
        "            \"output_tokens\": completion.usage.completion_tokens\n",
        "        }\n",
        "\n",
        "    def generate_continuation(self, story_so_far: str, current_step: int) -> List[str]:\n",
        "        \"\"\"Generate possible continuations for the story.\"\"\"\n",
        "        prompt = f\"\"\"You are a great storyteller and creative writer.\n",
        "You are creating act by act a synopsis of an original and engaging story.\n",
        "Based on the story setting and the existing part of the synopsis, your task is to concisely describe the next act of the story in an engaging and consistent way.\n",
        "It is very important for my career that the new act is a smooth continuation of the story so far.\n",
        "The story setting contains:\n",
        "- A location for the actions to take place (name and short description),\n",
        "- Details on three main characters: a propagonist, a protagonist's friend, and antagonist.\n",
        "  For each of them , generate their name, age, personal pronouns, occupation, and a description of an internal goal.\n",
        "- The nature of the conflict between the protagonist and the antagonist.\n",
        "This is the {current_step}-th act. It is crucial that the story is finalized in {self.max_steps} acts.\n",
        "THE STORY SETTING:\n",
        "{self.story_details}\n",
        "\n",
        "THE STORY SYNOPSIS SO FAR IS:\n",
        "\n",
        "{story_so_far}\n",
        "\n",
        "THE NEXT ACT:\n",
        "\"\"\"\n",
        "\n",
        "        completion = self.storyteller_client.chat.completions.create(\n",
        "            model=self.storyteller_model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt}\n",
        "                ],\n",
        "            max_tokens=500,     # Keep it controlled\n",
        "            temperature=0.6     # Keep it original\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"story_continuation\": completion.choices[0].message.content,\n",
        "            \"input_tokens\": completion.usage.prompt_tokens,\n",
        "            \"output_tokens\": completion.usage.completion_tokens\n",
        "        }\n",
        "\n",
        "    def generate_story(self) -> str:\n",
        "        \"\"\"Create a story step by step.\"\"\"\n",
        "\n",
        "        results = self.generate_beginning()\n",
        "        beginning = results[\"story_beginning\"]\n",
        "        total_input_tokens_generation = results[\"input_tokens\"]\n",
        "        total_output_tokens_generation = results[\"output_tokens\"]\n",
        "\n",
        "        print(\"\\n==============\\nStory beginning:\\n\")\n",
        "        print(beginning)\n",
        "        story = beginning\n",
        "\n",
        "        for step in range(self.max_steps):\n",
        "            print(f\"\\n==============\\nStep {step + 1}:\\n\")\n",
        "            results = self.generate_continuation(story, step + 1)\n",
        "            continuation = results[\"story_continuation\"]\n",
        "            total_input_tokens_generation += results[\"input_tokens\"]\n",
        "            total_output_tokens_generation += results[\"output_tokens\"]\n",
        "            story += '\\n'\n",
        "            story += continuation\n",
        "\n",
        "            print(continuation)\n",
        "\n",
        "        # Return final stories with their scores\n",
        "        return {\n",
        "            \"story\": story,\n",
        "            \"input_tokens_generation\": total_input_tokens_generation,\n",
        "            \"output_tokens_generation\": total_output_tokens_generation,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "sjwKwo-lS3i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storyteller_model = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
        "storyteller_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "step_by_step_story_generator = StepByStepStoryGenerator(\n",
        "     storyteller_client=storyteller_client, storyteller_model=storyteller_model,\n",
        "     story_details=setting,\n",
        "     max_steps=5\n",
        ")\n",
        "result = step_by_step_story_generator.generate_story()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiBzyoLvS3ls",
        "outputId": "9fdc71ed-3ffe-4d09-d62b-52471c44f683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============\n",
            "Story beginning:\n",
            "\n",
            "**Act 1: The Spark of Resistance**\n",
            "\n",
            "In the heart of Elderwood Grove, a lush and vibrant forest on the edge of a small town, 28-year-old environmental engineer Beaver has spent his entire career fighting to preserve the natural beauty of his home. His latest obsession is to create a revolutionary eco-friendly technology that can save the forest from the devastating effects of pollution.\n",
            "\n",
            "Beaver's friend and confidant, Arachnotron, a 26-year-old inventor and tinkerer, has been a constant source of support and creative inspiration. Together, they've been secretly working on a sustainable solution to the forest's environmental woes, but their efforts are about to be put to the ultimate test.\n",
            "\n",
            "Enter Rufus Grimly, a ruthless 35-year-old CEO of Grimly Industries, who will stop at nothing to expand his waste management empire and secure his legacy. Unbeknownst to Beaver and Arachnotron, Grimly has been secretly planning to build a toxic waste facility near Elderwood Grove, threatening to destroy the very ecosystem they're fighting to protect.\n",
            "\n",
            "As Beaver and Arachnotron stumble upon Grimly's sinister plans, they realize they must act quickly to rally the community and develop a sustainable alternative to the facility. But with Grimly's vast resources and influence, their fight will be far from easy. The stage is set for a battle between the forces of environmental destruction and the determination of two passionate individuals who refuse to give up on their vision of a greener, safer future.\n",
            "\n",
            "The first act of our story will follow Beaver and Arachnotron as they:\n",
            "\n",
            "* Discover Grimly's plans and sound the alarm to the community\n",
            "* Begin developing a sustainable alternative to the toxic waste facility\n",
            "* Create gadgets and tools with Arachnotron's inventive prowess to aid in their fight\n",
            "* Face off against Grimly's goons and intimidation tactics, setting the stage for a thrilling confrontation between the two opposing forces.\n",
            "\n",
            "The fate of Elderwood Grove hangs in the balance, and the clock is ticking. Will Beaver and Arachnotron be able to overcome the odds and save their beloved forest, or will Grimly's relentless pursuit of profit and power destroy everything they hold dear?\n",
            "\n",
            "==============\n",
            "Step 1:\n",
            "\n",
            "**Act 2: The Community Rises**\n",
            "\n",
            "As the news of Grimly's plans spreads like wildfire through Elderwood Grove, the community is galvanized into action. Beaver and Arachnotron's alarm has sounded, and the people are rising up to defend their home.\n",
            "\n",
            "The small town is filled with the sound of protests, rallies, and meetings, as residents from all walks of life come together to voice their opposition to the toxic waste facility. Beaver, ever the environmental engineer, takes center stage, using his expertise to explain the devastating consequences of the facility and the importance of a sustainable alternative.\n",
            "\n",
            "Arachnotron, meanwhile, uses their inventive talents to create a series of eye-catching gadgets and tools that help amplify the community's message. From giant banners to eco-friendly signs, their creations are a testament to their creativity and dedication to the cause.\n",
            "\n",
            "However, Grimly is not one to be underestimated. He unleashes a team of ruthless PR specialists, who begin to spin the story in his favor, painting Beaver and Arachnotron as \"eco-terrorists\" and \" obstructionists\" who are standing in the way of progress.\n",
            "\n",
            "As tensions rise, Beaver and Arachnotron find themselves facing increasing pressure from all sides. They must navigate the complex web of community politics, while also keeping their focus on developing a viable alternative to the toxic waste facility.\n",
            "\n",
            "Meanwhile, a mysterious figure begins to emerge from the shadows, offering to provide Beaver and Arachnotron with crucial information about Grimly's operations and the inner workings of his empire. But can they trust this enigmatic ally, or is it just another ploy to further their own interests?\n",
            "\n",
            "The stakes are higher than ever, as the fate of Elderwood Grove hangs in the balance. Will Beaver and Arachnotron be able to rally the community and develop a sustainable solution, or will Grimly's forces crush their spirits and destroy the forest they love? The battle rages on, and the outcome is far from certain.\n",
            "\n",
            "==============\n",
            "Step 2:\n",
            "\n",
            "**Act 3: The Dark Horse**\n",
            "\n",
            "As the community continues to rally behind Beaver and Arachnotron, a new player emerges from the shadows, threatening to upend the delicate balance of power. Enter Maya Blackwood, a reclusive and enigmatic billionaire who has been secretly funding environmental causes for years.\n",
            "\n",
            "Maya reveals to Beaver and Arachnotron that she has been following their work and is impressed by their dedication and ingenuity. She offers to provide them with significant financial support and resources, but at a steep price: she wants a 20% stake in their sustainable technology, and demands that they partner with her company, Blackwood Enterprises, to bring their solution to market.\n",
            "\n",
            "Arachnotron is immediately suspicious of Maya's motives, fearing that she may be using them to further her own business interests rather than genuinely supporting their cause. Beaver, however, is tempted by the prospect of securing the necessary funding to bring their technology to scale.\n",
            "\n",
            "As the two friends debate the pros and cons of partnering with Maya, Grimly sees an opportunity to exploit their divisions. He begins to spread rumors that Maya is a \"greenwasher\" who only cares about making a profit, and that Beaver and Arachnotron are being manipulated by her.\n",
            "\n",
            "The community is torn, with some members supporting Beaver's decision to partner with Maya, while others, like Arachnotron, are adamant that they should remain independent. Meanwhile, Grimly's PR machine goes into overdrive, painting Maya as a \"shadowy figure\" with ulterior motives.\n",
            "\n",
            "Beaver and Arachnotron must navigate this treacherous landscape, making a choice that will determine the fate of their technology, their partnership, and the future of Elderwood Grove. Will they take the risk and partner with Maya, or will they stick to their guns and remain independent? The consequences of their decision will be far-reaching, and the outcome is far from certain.\n",
            "\n",
            "==============\n",
            "Step 3:\n",
            "\n",
            "**Act 4: The Turning Point**\n",
            "\n",
            "As the debate over partnering with Maya reaches a boiling point, Beaver and Arachnotron find themselves at a crossroads. They must make a choice that will determine the fate of their technology, their partnership, and the future of Elderwood Grove.\n",
            "\n",
            "Beaver, tempted by the prospect of securing the necessary funding, begins to lean towards partnering with Maya. He sees it as an opportunity to bring their technology to scale and make a real difference in the world. Arachnotron, however, remains skeptical, fearing that Maya's true intentions are to exploit their work for her own gain.\n",
            "\n",
            "As the tension between them reaches a breaking point, a shocking revelation throws everything into chaos. Maya's true intentions are revealed, and it becomes clear that she has been playing a long game all along. Her offer of funding and support was not what it seemed, and Beaver and Arachnotron's technology is now at the center of a much larger, more complex web of interests.\n",
            "\n",
            "With this new information, Beaver and Arachnotron must re-evaluate their partnership and their goals. They must decide whether to continue down the path of working with Maya, or to take a stand and remain independent. But as they make this choice, they realize that the stakes have never been higher. Grimly's forces are closing in, and the community is on the brink of collapse.\n",
            "\n",
            "As the fate of Elderwood Grove hangs in the balance, Beaver and Arachnotron must confront the consequences of their actions and make a choice that will determine the course of their lives and the future of the forest. Will they take the risk and stand up for what they believe in, or will they play it safe and partner with Maya? The outcome is far from certain, and the clock is ticking.\n",
            "\n",
            "**The stage is set for a thrilling conclusion, as Beaver and Arachnotron face their greatest challenge yet. Will they emerge victorious, or will the forces of greed and destruction prevail? The fate of Elderwood Grove hangs in the balance, and the outcome is far from certain.**\n",
            "\n",
            "==============\n",
            "Step 4:\n",
            "\n",
            "**Act 5: The Final Stand**\n",
            "\n",
            "As Beaver and Arachnotron grapple with the consequences of Maya's revelation, the community is on the brink of collapse. Grimly's forces have gained the upper hand, and the toxic waste facility is on the verge of being approved.\n",
            "\n",
            "Determined to take a stand, Beaver and Arachnotron rally the community and launch a final, desperate bid to stop the facility. With the help of their friends and allies, they organize a massive protest, using all the creative and inventive skills they've developed throughout the story to make their voices heard.\n",
            "\n",
            "Meanwhile, Maya's true intentions are exposed, and her company is revealed to be secretly backing Grimly's plans all along. The community is shocked and outraged, and Beaver and Arachnotron must now confront the fact that they were nearly played by a master manipulator.\n",
            "\n",
            "As the protest reaches its climax, Beaver and Arachnotron face off against Grimly in a final, decisive confrontation. With the fate of Elderwood Grove hanging in the balance, they must use all their skills, ingenuity, and determination to stop the toxic waste facility and secure a sustainable future for their beloved forest.\n",
            "\n",
            "In a thrilling and emotional climax, the battle between the forces of environmental destruction and the determination of two passionate individuals comes to a head. Will Beaver and Arachnotron emerge victorious, or will the forces of greed and destruction prevail? The fate of Elderwood Grove hangs in the balance, and the outcome is far from certain.\n",
            "\n",
            "In the end, Beaver and Arachnotron's courage, creativity, and determination pay off. They successfully stop the toxic waste facility, and the community comes together to rebuild and restore the forest. Maya's true intentions are exposed, and her company is brought to justice.\n",
            "\n",
            "As the dust settles, Beaver and Arachnotron are hailed as heroes, their names etched in the annals of history as champions of the environment and defenders of the natural world. The story concludes on a hopeful note, with the promise of a brighter, more sustainable future for Elderwood Grove and all the creatures that call it home.\n",
            "\n",
            "The final scene shows Beaver and Arachnotron standing together, looking out over the restored forest, their eyes shining with a sense of pride, accomplishment, and hope for the future. The camera pans out, showing the vibrant, thriving ecosystem of Elderwood Grove, a testament to the power of courage, creativity, and determination in the face of adversity\n",
            "\n",
            "==============\n",
            "Step 5:\n",
            "\n",
            "**Act 5: The Final Stand**\n",
            "\n",
            "As Beaver and Arachnotron grapple with the consequences of Maya's revelation, the community is on the brink of collapse. Grimly's forces have gained the upper hand, and the toxic waste facility is on the verge of being approved.\n",
            "\n",
            "Determined to take a stand, Beaver and Arachnotron rally the community and launch a final, desperate bid to stop the facility. With the help of their friends and allies, they organize a massive protest, using all the creative and inventive skills they've developed throughout the story to make their voices heard.\n",
            "\n",
            "Meanwhile, Maya's true intentions are exposed, and her company is revealed to be secretly backing Grimly's plans all along. The community is shocked and outraged, and Beaver and Arachnotron must now confront the fact that they were nearly played by a master manipulator.\n",
            "\n",
            "As the protest reaches its climax, Beaver and Arachnotron face off against Grimly in a final, decisive confrontation. With the fate of Elderwood Grove hanging in the balance, they must use all their skills, ingenuity, and determination to stop the toxic waste facility and secure a sustainable future for their beloved forest.\n",
            "\n",
            "In a thrilling and emotional climax, the battle between the forces of environmental destruction and the determination of two passionate individuals comes to a head. Will Beaver and Arachnotron emerge victorious, or will the forces of greed and destruction prevail? The fate of Elderwood Grove hangs in the balance, and the outcome is far from certain.\n",
            "\n",
            "In the end, Beaver and Arachnotron's courage, creativity, and determination pay off. They successfully stop the toxic waste facility, and the community comes together to rebuild and restore the forest. Maya's true intentions are exposed, and her company is brought to justice.\n",
            "\n",
            "As the dust settles, Beaver and Arachnotron are hailed as heroes, their names etched in the annals of history as champions of the environment and defenders of the natural world. The story concludes on a hopeful note, with the promise of a brighter, more sustainable future for Elderwood Grove and all the creatures that call it home.\n",
            "\n",
            "The final scene shows Beaver and Arachnotron standing together, looking out over the restored forest, their eyes shining with a sense of pride, accomplishment, and hope for the future. The camera pans out, showing the vibrant, thriving ecosystem of Elderwood Grove, a testament to the power of courage, creativity, and determination in the face of adversity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating story originality using LLM as a judge\n",
        "\n",
        "LLMs' efforts at creative writing often lack originality, both in plot and style. In this part, we'll try to evaluate originality of a plot fragment.\n",
        "\n",
        "**Task 3.3.** Create a function `evaluate_story` to score originality of the Llama's generation on 1 to 5 scale. Some points to think about:\n",
        "\n",
        "- How would you define originality for yourself?\n",
        "- Don't forget to write the meaning of each of the grades 1-5. Like:\n",
        "\n",
        "```\n",
        "Here is the scale you should use to build your answer. Closely follow this scale while answering.\n",
        "1 — Very Unoriginal\n",
        "<YOUR DESCRIPTION>\n",
        "\n",
        "2 — Slightly Unoriginal\n",
        "<YOUR DESCRIPTION>\n",
        "\n",
        "3 — Moderately Original\n",
        "<YOUR DESCRIPTION>\n",
        "\n",
        "4 — Quite Original\n",
        "<YOUR DESCRIPTION>\n",
        "\n",
        "5 — Highly Original\n",
        "<YOUR DESCRIPTION>\n",
        "```\n",
        "\n",
        "The better you describe your notion of originality, the higher the chance that the LLM will understand it.\n",
        "\n",
        "We recommend using a more powerful model for evaluation. For example, **gpt-4o-mini**. By the way, it may be curious to compare how gpt-4o-mini evaluates its own creations vs Llama's creations. Sometimes LLMs exhibit preference towards their own generation (this is known as **self-enhancement bias**).\n",
        "\n",
        "Also, it may be beneficial to ask the judge LLM not only to output rating, but also to provide a justification. This way you'll need an extraction call to get the rating, of course.\n",
        "\n",
        "Experiment with inputs. Compare which scores Llama-3.1-8B will get on average with average rankings of, say, summaries of your favourite writer's chapters. How much the outputs aligh with your understanding of originality?\n",
        "\n",
        "Two posts to get inspiration about using LLM as a judge:\n",
        "\n",
        "* [https://huggingface.co/learn/cookbook/llm_judge](https://huggingface.co/learn/cookbook/llm_judge)\n",
        "* [https://www.evidentlyai.com/llm-guide/llm-as-a-judge](https://www.evidentlyai.com/llm-guide/llm-as-a-judge)"
      ],
      "metadata": {
        "id": "VCT1HO4ayq8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story_fragment = \"\"\"**Act 2: The Community Transmutates**\n",
        "\n",
        "As the news of Grimly's plans infects Elderwood Grove like a fungal spore, the community undergoes a profound metamorphosis. Beaver and Arachnotron's alarm has awakened a sentient network of ancient forest energies, and the people are becoming vessels for the land's own resistance.\n",
        "\n",
        "Protests, rallies, and meetings are supplanted by surreal, ritual-like gatherings, where residents don masks made of twisted vines and whispers of the forest are amplified through primal chanting. Beaver, now an unwitting shaman, channels the voices of the forest, conjuring visions of a blasted wasteland should the toxic facility be built.\n",
        "\n",
        "Arachnotron's workshop births an assortment of bioluminescent contraptions that blend the lines between technology and nature. The creations are an adequate testament to their ingenuity and willingness to transcend human limitations in service to the land.\n",
        "\n",
        "However, Grimly has assembled a cabal of cyber-witch PR specialists who begin to condition the narrative to favor the toxic facility, framing Beaver and Arachnotron as disruptors of the natural order, opposed to the sacred engine of progress.\n",
        "\n",
        "As community fault lines expand, Beaver and Arachnotron become the focal points of clashing elemental forces. A veiled messenger manifests, claiming to bring forbidden knowledge regarding Grimly's labyrinthine empire and promising to awaken hidden symmetries. But will Beaver and Arachnotron submit to this eerie guide, potentially disorienting themselves to uncover the hidden blueprint to Elderwood Grove's survival?\n",
        "\n",
        "Elderwood Grove totters at the edge of becoming a sacred or desolate place, as reality-makers embark on a firmly-looking battle that recasts destinies, aligns loyalties, and portends possibilities to reorder existence.\"\"\"\n",
        "\n",
        "evaluation_prompt = f\"\"\"You are a professional book critic and a writing coach.\n",
        "You will be given a fragment from a short story.\n",
        "Your task is to evaluate how much original is the story.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that fragment is very conventional, as if generated by an LLM, and 5 means that the fragment seems very original.\n",
        "\n",
        "Here is the scale you should use to build your answer. Closely follow this scale while answering.\n",
        "1 — Very Unoriginal\n",
        "The plot is extremely predictable, following well-worn tropes or clichés with no unique twists or perspectives.\n",
        "\n",
        "2 — Slightly Unoriginal\n",
        "The plot contains some originality but relies heavily on common themes or ideas; few elements feel fresh or distinct.\n",
        "\n",
        "3 — Moderately Original\n",
        "The plot shows a fair balance between familiar elements and unique ideas; some twists are present, though aspects remain predictable.\n",
        "\n",
        "4 — Quite Original\n",
        "The plot is mostly unique, with creative twists, unusual perspectives, or distinctive themes, making it stand out from typical stories.\n",
        "\n",
        "5 — Highly Original\n",
        "The plot is exceptionally original, with inventive ideas, surprising twists, and a fresh approach that defies standard expectations.\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "Feedback:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "STORY FRAGMENT:\n",
        "{story_fragment}\n",
        "END OF STORY FRAGMENT\n",
        "\n",
        "Provide your feedback. If you give a correct rating, I'll bake you a pie.\n",
        "Feedback:::\n",
        "Evaluation: \"\"\"\n",
        "\n",
        "evaluation_model=\"gpt-4o-mini\"\n",
        "\n",
        "evaluation_completion = openai.chat.completions.create(\n",
        "            messages=[\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": evaluation_prompt\n",
        "                }\n",
        "            ],\n",
        "            model=evaluation_model,\n",
        "            )\n",
        "evaluation = evaluation_completion.choices[0].message.content\n",
        "evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "94y0xNJiS3te",
        "outputId": "5fa59fb0-6bbd-4480-9694-e491f6e1a4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Evaluation: The story fragment exhibits a high degree of creativity and originality, particularly through its imaginative blend of nature and technology, as well as its unique setting in Elderwood Grove. The characterizations of Beaver and Arachnotron as shamanistic figures who channel the forest’s energies stands out, signaling a fresh perspective on the themes of environmental resistance and community action. The ritual-like gatherings and the surreal elements add layers of intrigue, while the narrative conflict between Grimly’s toxic facility and the community's uprising presents a compelling dichotomy. However, some tropes—such as the challenge against a corporate antagonist—are familiar, which slightly tempers its uniqueness. Overall, it leans more toward originality due to its distinctive narrative voice and conceptual depth.\\n\\nTotal rating: 4\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also try rewriting the story to make it more original."
      ],
      "metadata": {
        "id": "pPDm7EVnhmog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story_fragment = \"\"\"**Act 2: The Community Rises**\n",
        "\n",
        "As the news of Grimly's plans spreads like wildfire through Elderwood Grove, the community is galvanized into action. Beaver and Arachnotron's alarm has sounded, and the people are rising up to defend their home.\n",
        "\n",
        "The small town is filled with the sound of protests, rallies, and meetings, as residents from all walks of life come together to voice their opposition to the toxic waste facility. Beaver, ever the environmental engineer, takes center stage, using his expertise to explain the devastating consequences of the facility and the importance of a sustainable alternative.\n",
        "\n",
        "Arachnotron, meanwhile, uses their inventive talents to create a series of eye-catching gadgets and tools that help amplify the community's message. From giant banners to eco-friendly signs, their creations are a testament to their creativity and dedication to the cause.\n",
        "\n",
        "However, Grimly is not one to be underestimated. He unleashes a team of ruthless PR specialists, who begin to spin the story in his favor, painting Beaver and Arachnotron as \"eco-terrorists\" and \" obstructionists\" who are standing in the way of progress.\n",
        "\n",
        "As tensions rise, Beaver and Arachnotron find themselves facing increasing pressure from all sides. They must navigate the complex web of community politics, while also keeping their focus on developing a viable alternative to the toxic waste facility.\n",
        "\n",
        "Meanwhile, a mysterious figure begins to emerge from the shadows, offering to provide Beaver and Arachnotron with crucial information about Grimly's operations and the inner workings of his empire. But can they trust this enigmatic ally, or is it just another ploy to further their own interests?\n",
        "\n",
        "The stakes are higher than ever, as the fate of Elderwood Grove hangs in the balance. Will Beaver and Arachnotron be able to rally the community and develop a sustainable solution, or will Grimly's forces crush their spirits and destroy the forest they love? The battle rages on, and the outcome is far from certain.\"\"\"\n",
        "\n",
        "\n",
        "rewriting_prompt = f\"\"\"You are a professional book critic and a writing coach.\n",
        "You will be given a fragment from a short story.\n",
        "You task is to rewrite it for more originality, introducing inventive ideas, surprising twists, deep personal conflicts, and a fresh approach that defies standard expectations.\n",
        "Do keep the fragment concise after rewriting!\n",
        "\n",
        "STORY FRAGMENT:\n",
        "{story_fragment}\n",
        "END OF STORY FRAGMENT\n",
        "\n",
        "Only output the rewritten fragment. If you do it correctly, I'll give you a huge GPU grant.\n",
        "REWRITTEN VERSION:\n",
        "\"\"\"\n",
        "\n",
        "rewriting_model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
        "rewriting_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "rewriting_completion = rewriting_client.chat.completions.create(\n",
        "            messages=[\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": rewriting_prompt\n",
        "                }\n",
        "            ],\n",
        "            model=rewriting_model,\n",
        "            )\n",
        "rewriting = rewriting_completion.choices[0].message.content\n",
        "rewriting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "EaTMNmRzwYgU",
        "outputId": "4233e83c-3dec-4655-8a66-ed45f51da046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"**Act 2: The Community Transmutates**\\n\\nAs the news of Grimly's plans infects Elderwood Grove like a fungal spore, the community undergoes a profound metamorphosis. Beaver and Arachnotron's alarm has awakened a sentient network of ancient forest energies, and the people are becoming vessels for the land's own resistance.\\n\\nProtests, rallies, and meetings are supplanted by surreal, ritual-like gatherings, where residents don masks made of twisted vines and whispers of the forest are amplified through primal chanting. Beaver, now an unwitting shaman, channels the voices of the forest, conjuring visions of a blasted wasteland should the toxic facility be built.\\n\\nArachnotron's workshop births an assortment of bioluminescent contraptions that blend the lines between technology and nature. The creations are an adequate testament to their ingenuity and willingness to transcend human limitations in service to the land.\\n\\nHowever, Grimly has assembled a cabal of cyber-witch PR specialists who begin to condition the narrative to favor the toxic facility, framing Beaver and Arachnotron as disruptors of the natural order, opposed to the sacred engine of progress.\\n\\nAs community fault lines expand, Beaver and Arachnotron become the focal points of clashing elemental forces. A veiled messenger manifests, claiming to bring forbidden knowledge regarding Grimly's labyrinthine empire and promising to awaken hidden symmetries. But will Beaver and Arachnotron submit to this eerie guide, potentially disorienting themselves to uncover the hidden blueprint to Elderwood Grove's survival?\\n\\nElderwood Grove totters at the edge of becoming a sacred or desolate place, as reality-makers embark on a firmly-looking battle that recasts destinies, aligns loyalties, and portends possibilities to reorder existence.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam Search (bonus part for those brave enough)\n",
        "\n",
        "Now that we decomposed story generation into clear steps, we can use some of the beyond-CoT methodology to make it better.\n",
        "\n",
        "Our first ingredient will be stepwise scoring (or, speaking in clever terms, **process reward**). In real-life situation I'd suggest fine tuning a BERT-like model on LLM-as-a-Judge originality scores. BERT only predicts the answer, so it's way cheaper than using LLM as a Judge. But we don't serve models ourselves just yes, and thus we'll continue using gpt-4o-mini-based evaluation for our toy example.\n",
        "\n",
        "The second ingredient is an algorithm of optimal reasoning path search. We already know one example - **self-consistency**, which suggests just running several reasoning paths in parallel and choosing the one which eventually arrives at the best answer. But self-consistency doesn't leverage process rewards. So, we are tempted to use Tree of Thoughts or an even more complicated algorithm, but I suggest using **Beam Search**, which strikes a good balance between improvement and simplicity.\n",
        "\n",
        "**Beam search** with **beam size** B works like this:\n",
        "\n",
        "1. Generate B starting steps, put them in a beam,\n",
        "2. Do until we converge or reach max steps:\n",
        "\n",
        "  a. From each partial solution in the beam, generate B next steps,\n",
        "  b. Score each of the B${}^2$ solutions we get, choose B best ones.\n",
        "\n",
        "This way, on each step we keep only B best solutions.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1y3io1RqfqyIKcfu7kxRIiZKNKGMXtt9P\" width=600 /></center>"
      ],
      "metadata": {
        "id": "gFQxgL1Z5Czr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**My solution**. I'm also asking Llama-405B to rewrite the first act in hope that this will make the further plot more original as well. Even though the style is still very recognizable, at least the text became eventually a little less cliché."
      ],
      "metadata": {
        "id": "AJsQ6V2LmE3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "import heapq\n",
        "import numpy as np\n",
        "\n",
        "class BeamSearchStoryGenerator:\n",
        "    def __init__(self,\n",
        "                 storyteller_client, storyteller_model,\n",
        "                 evaluation_client, evaluation_model,\n",
        "                 rewriting_client, rewriting_model,\n",
        "                 story_details: StoryDetails,\n",
        "                 beam_width: int = 2, max_steps: int = 5):\n",
        "        self.storyteller_client = storyteller_client\n",
        "        self.storyteller_model = storyteller_model\n",
        "        self.evaluation_client = evaluation_client\n",
        "        self.evaluation_model = evaluation_model\n",
        "        self.rewriting_client = rewriting_client\n",
        "        self.rewriting_model = rewriting_model\n",
        "        self.story_details = story_details\n",
        "        self.beam_width = beam_width\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "    def generate_beginnings(self) -> List[str]:\n",
        "        \"\"\"Generate possible starting fragments for the story.\"\"\"\n",
        "        prompt = f\"\"\"You are a great storyteller and creative writer.\n",
        "Based on the story setting, your task is to create a concise synopsis description of the first act of an original and engaging story.\n",
        "The story setting contains:\n",
        "- A location for the actions to take place (name and short description),\n",
        "- Details on three main characters: a propagonist, a protagonist's friend, and antagonist.\n",
        "  For each of them , generate their name, age, personal pronouns, occupation, and a description of an internal goal.\n",
        "- The nature of the conflict between the protagonist and the antagonist.\n",
        "THE STORY SETTING:\n",
        "{self.story_details}\n",
        "\n",
        "THE SYNOPSIS BEGINS:\n",
        "\"\"\"\n",
        "\n",
        "        completions = self.storyteller_client.chat.completions.create(\n",
        "            model=self.storyteller_model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt}\n",
        "                ],\n",
        "            n=self.beam_width,  # Get multiple completions\n",
        "            max_tokens=500,     # Keep it controlled\n",
        "            temperature=0.6     # Keep it original\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"story_beginnings\": [completion.message.content for completion in completions.choices],\n",
        "            \"input_tokens\": completions.usage.prompt_tokens,\n",
        "            \"output_tokens\": completions.usage.completion_tokens\n",
        "        }\n",
        "\n",
        "    def rewrite_beginning(self, story_fragment: str) -> List[str]:\n",
        "        \"\"\"Rewrite the beginnings using a larger model.\"\"\"\n",
        "        prompt = f\"\"\"You are a professional book critic and a writing coach.\n",
        "You will be given a fragment from a short story.\n",
        "You task is to rewrite it for more originality, introducing inventive ideas, surprising twists, deep personal conflicts, and a fresh approach that defies standard expectations.\n",
        "Do keep the fragment concise after rewriting!\n",
        "\n",
        "STORY FRAGMENT:\n",
        "{story_fragment}\n",
        "END OF STORY FRAGMENT\n",
        "\n",
        "Only output the rewritten fragment. If you do it correctly, I'll give you a huge GPU grant.\n",
        "REWRITTEN VERSION:\n",
        "\"\"\"\n",
        "\n",
        "        completion = self.rewriting_client.chat.completions.create(\n",
        "            model=self.rewriting_model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt}\n",
        "                ],\n",
        "            max_tokens=500,     # Keep it controlled\n",
        "            temperature=0.6     # Keep it original\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"rewritten_beginning\": completion.choices[0].message.content,\n",
        "            \"input_tokens\": completion.usage.prompt_tokens,\n",
        "            \"output_tokens\": completion.usage.completion_tokens\n",
        "        }\n",
        "\n",
        "    def generate_continuation(self, story_so_far: str, current_step:int) -> List[str]:\n",
        "        \"\"\"Generate possible continuations for the story.\"\"\"\n",
        "        prompt = f\"\"\"You are a great storyteller and creative writer.\n",
        "You are creating act by act a synopsis of an original and engaging story.\n",
        "Based on the story setting and the existing part of the synopsis, your task is to concisely describe the next act of the story in an engaging and consistent way.\n",
        "It is very important for my career that the new act is a smooth continuation of the story so far.\n",
        "The story setting contains:\n",
        "- A location for the actions to take place (name and short description),\n",
        "- Details on three main characters: a propagonist, a protagonist's friend, and antagonist.\n",
        "  For each of them , generate their name, age, personal pronouns, occupation, and a description of an internal goal.\n",
        "- The nature of the conflict between the protagonist and the antagonist.\n",
        "This is the {current_step}-th act. It is crucial that the story is finalized in {self.max_steps} acts.\n",
        "THE STORY SETTING:\n",
        "{self.story_details}\n",
        "\n",
        "THE STORY SYNOPSIS SO FAR IS:\n",
        "\n",
        "{story_so_far}\n",
        "\n",
        "THE NEXT ACT:\n",
        "\"\"\"\n",
        "\n",
        "        completions = self.storyteller_client.chat.completions.create(\n",
        "            model=self.storyteller_model,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt}\n",
        "                ],\n",
        "            n=self.beam_width,  # Get multiple completions\n",
        "            max_tokens=500,     # Keep it controlled\n",
        "            temperature=0.6     # Keep it original\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"story_continuations\": [completion.message.content for completion in completions.choices],\n",
        "            \"input_tokens\": completions.usage.prompt_tokens,\n",
        "            \"output_tokens\": completions.usage.completion_tokens\n",
        "        }\n",
        "\n",
        "    def evaluate_story(self, story_so_far: str) -> float:\n",
        "        \"\"\"Evaluate the originality of the story using LLM as judge.\"\"\"\n",
        "        prompt = f\"\"\"You are a professional book critic and a writing coach.\n",
        "You will be given a fragment from a short story.\n",
        "Your task is to evaluate how much original is the story.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that fragment is very conventional, as if generated by an LLM, and 5 means that the fragment seems very original.\n",
        "\n",
        "Here is the scale you should use to build your answer. Closely follow this scale while answering.\n",
        "1 — Very Unoriginal\n",
        "The plot is extremely predictable, following well-worn tropes or clichés with no unique twists or perspectives.\n",
        "\n",
        "2 — Slightly Unoriginal\n",
        "The plot contains some originality but relies heavily on common themes or ideas; few elements feel fresh or distinct.\n",
        "\n",
        "3 — Moderately Original\n",
        "The plot shows a fair balance between familiar elements and unique ideas; some twists are present, though aspects remain predictable.\n",
        "\n",
        "4 — Quite Original\n",
        "The plot is mostly unique, with creative twists, unusual perspectives, or distinctive themes, making it stand out from typical stories.\n",
        "\n",
        "5 — Highly Original\n",
        "The plot is exceptionally original, with inventive ideas, surprising twists, and a fresh approach that defies standard expectations.\n",
        "\n",
        "Provide your feedback as follows:\n",
        "\n",
        "Feedback:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "STORY FRAGMENT:\n",
        "{story_fragment}\n",
        "END OF STORY FRAGMENT\n",
        "\n",
        "Provide your feedback. If you give a correct rating, I'll bake you a pie.\n",
        "Feedback:::\n",
        "Evaluation: \"\"\"\n",
        "\n",
        "        evaluation_completion = self.evaluation_client.chat.completions.create(\n",
        "            model=self.evaluation_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        explained_score = evaluation_completion.choices[0].message.content\n",
        "\n",
        "        extraction_completion = self.evaluation_client.chat.completions.create(\n",
        "            model=self.evaluation_model,\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"You are a professional score extractor.\n",
        "You are given several evaluations of stories, each of which justifies an integer rating from 1 to 5:\n",
        "This rating is mentioned under as Total rating and has values 1, 2, 3, 4, or 5.\n",
        "You output only one number from 1 to 5, the Total rating.\n",
        "\n",
        "EVALUATION:\n",
        "The story has some interesting elements, such as Benny the beaver and the mystical realm of The GPT land. However, the narrative becomes increasingly disjointed and confusing from the midpoint onwards. The language and sentence structure deteriorate, with sentence fragments and unrelated words and phrases inserted throughout the story. This makes it difficult for the reader to follow and engage with the story. Despite the initial promise of a grand tale, the story's potential is ultimately undermined by its inconsistent and confusing conclusion.\n",
        "\n",
        "Total rating: 2\n",
        "\"\"\"},\n",
        "                      {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\"\"2\"\"\"},\n",
        "                      {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"EVALUATION:\n",
        "{explained_score}\n",
        "\"\"\"},],\n",
        "        )\n",
        "\n",
        "        total_rating_raw = extraction_completion.choices[0].message.content\n",
        "        if total_rating_raw[0] in ['1', '2', '3', '4', '5']:\n",
        "            total_rating = int(total_rating_raw[0])\n",
        "        elif total_rating_raw[-1] in ['1', '2', '3', '4', '5']:\n",
        "            total_rating = int(total_rating_raw[-1])\n",
        "        else:\n",
        "            print(f\"\"\"\n",
        "                === UNINTELLIGIBLE RATING ===\n",
        "                {total_rating_raw}\n",
        "            \"\"\")\n",
        "            total_rating = np.random.randint(1, 6)\n",
        "\n",
        "        return {\n",
        "            \"total_rating\": total_rating,\n",
        "            \"input_tokens\": evaluation_completion.usage.prompt_tokens + extraction_completion.usage.prompt_tokens,\n",
        "            \"output_tokens\": evaluation_completion.usage.completion_tokens + extraction_completion.usage.completion_tokens\n",
        "        }\n",
        "\n",
        "    def beam_search(self) -> List[Tuple[float, str]]:\n",
        "        \"\"\"Perform beam search for story generation.\"\"\"\n",
        "        # Initialize beam with the initial prompt\n",
        "\n",
        "        results = self.generate_beginnings()\n",
        "        beginnings = results[\"story_beginnings\"]\n",
        "        total_input_tokens_generation = results[\"input_tokens\"]\n",
        "        total_output_tokens_generation = results[\"output_tokens\"]\n",
        "\n",
        "        current_beam = []\n",
        "        total_input_tokens_rewriting = 0\n",
        "        total_output_tokens_rewriting = 0\n",
        "        for beginning in beginnings:\n",
        "            result = self.rewrite_beginning(beginning)\n",
        "            current_beam.append((0.0, result['rewritten_beginning']))\n",
        "            total_input_tokens_rewriting = result['input_tokens']\n",
        "            total_output_tokens_rewriting = result['output_tokens']\n",
        "\n",
        "        total_input_tokens_evaluation = 0\n",
        "        total_output_tokens_evaluation = 0\n",
        "\n",
        "        for step in range(self.max_steps):\n",
        "            print(f\"\\n==============\\nStep {step + 1}:\\n\")\n",
        "            candidates = []\n",
        "\n",
        "            # Generate continuations for each story in current beam\n",
        "            for score, story in current_beam:\n",
        "                results = self.generate_continuation(story, step + 1)\n",
        "                continuations = results[\"story_continuations\"]\n",
        "                total_input_tokens_generation += results[\"input_tokens\"]\n",
        "                total_output_tokens_generation += results[\"output_tokens\"]\n",
        "\n",
        "                # Evaluate each continuation\n",
        "                for continuation in continuations:\n",
        "                    new_story = f\"{story}\\n {continuation}\"\n",
        "                    scoring_result = self.evaluate_story(new_story)\n",
        "                    new_score = scoring_result[\"total_rating\"]\n",
        "                    total_input_tokens_evaluation += scoring_result[\"input_tokens\"]\n",
        "                    total_output_tokens_evaluation += scoring_result[\"output_tokens\"]\n",
        "                    candidates.append((-new_score, new_story))  # Negative for max-heap\n",
        "                    print(f\"Candidate (score {new_score:.2f}):\\n{new_story}\\n\")\n",
        "\n",
        "            # Select top-k candidates for next beam\n",
        "            heapq.heapify(candidates)\n",
        "            current_beam = [(-score, story) for score, story in heapq.nsmallest(self.beam_width, candidates)]\n",
        "\n",
        "            print(\"Selected for next beam:\")\n",
        "            for score, story in current_beam:\n",
        "                print(f\"Score: {score:.2f}\\n{story}\\n\")\n",
        "\n",
        "        # Return final stories with their scores\n",
        "        return {\n",
        "            \"stories\": [(score, story) for score, story in current_beam],\n",
        "            \"input_tokens_generation\": total_input_tokens_generation,\n",
        "            \"output_tokens_generation\": total_output_tokens_generation,\n",
        "            \"input_tokens_evaluation\": total_input_tokens_evaluation,\n",
        "            \"output_tokens_evaluation\": total_output_tokens_evaluation\n",
        "        }\n"
      ],
      "metadata": {
        "id": "QuyfAoEjgh9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nebius_client = storyteller_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "storyteller_client = nebius_client\n",
        "rewriting_client = nebius_client\n",
        "evaluation_client = openai\n",
        "\n",
        "storyteller_model = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
        "rewriting_model = 'meta-llama/Meta-Llama-3.1-405B-Instruct'\n",
        "evaluation_model = 'gpt-4o-mini'"
      ],
      "metadata": {
        "id": "JWkw_avdBQjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search_story_generator = BeamSearchStoryGenerator(\n",
        "     storyteller_client=storyteller_client, storyteller_model=storyteller_model,\n",
        "     rewriting_client=rewriting_client, rewriting_model=rewriting_model,\n",
        "     evaluation_client=evaluation_client, evaluation_model=evaluation_model,\n",
        "     story_details=setting,\n",
        "     beam_width=2,\n",
        "     max_steps=5\n",
        ")\n",
        "result = beam_search_story_generator.beam_search()"
      ],
      "metadata": {
        "id": "yoAK5hXdgiAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"stories\"][0][0]) # rating\n",
        "print(result[\"stories\"][0][1]) # story"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyLKy4UNgehP",
        "outputId": "35cc14b1-21f5-43da-c35e-101b9270331b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "**Act 1: \"The Shadows of Elderwood Grove\"**\n",
            "\n",
            "In the depths of Elderwood Grove, a forest teeming with life on the cusp of a small, rural town, Environmental Engineer Beaver (28, he/him) grapples with the legacy of his late grandmother, a renowned eco-activist who vanished under mysterious circumstances. Her cryptic journals hint at an ancient, symbiotic relationship between the forest and its inhabitants – one that Beaver is determined to unlock through his innovative eco-friendly technology. This mission is fueled by his need to understand his grandmother's disappearance and to prove himself as a worthy successor to her environmental crusade.\n",
            "\n",
            "Beaver's closest ally, Arachnotron (26, they/them), a brilliant Inventor and Tinkerer, harbors a secret: they're an artificial intelligence created from the neural networks of the forest's creatures. As they navigate their existence, Arachnotron seeks to redefine the boundaries between technology and nature, and to find their place within the world. Their symbiotic connection to the forest grants them unparalleled insight into the ecosystem, allowing them to craft gadgets that blur the lines between organic and synthetic.\n",
            "\n",
            "However, the arrival of Rufus Grimly (35, he/him), the cunning CEO of Grimly Industries, sets off a catastrophic chain reaction. Grimly's true intentions are shrouded in mystery, but his plans for a 'revolutionary waste management facility' in Elderwood Grove are merely a smokescreen. He's searching for a specific, genetically engineered species rumored to thrive within the forest – one that holds the key to groundbreaking, eco-friendly technologies. This discovery would not only secure Grimly's legacy but also grant him unparalleled control over the global eco-industry.\n",
            "\n",
            "As Beaver and Arachnotron begin to unravel the mysteries of Grimly's plans, they must confront the darkness within their own pasts and the true cost of their environmental activism. The fate of Elderwood Grove hangs in the balance, and the consequences of their actions will forever alter the delicate balance between technology, nature, and humanity.\n",
            " **Act 2: \"The Web of Deception\"**\n",
            "\n",
            "As Beaver and Arachnotron delve deeper into Rufus Grimly's plans, they discover a hidden laboratory within the heart of Elderwood Grove, where Grimly's scientists are conducting clandestine experiments on the genetically engineered species. The duo must navigate the treacherous underbrush of the forest to gather evidence of Grimly's wrongdoing, all while avoiding the CEO's henchmen.\n",
            "\n",
            "Meanwhile, Arachnotron's connection to the forest's neural networks reveals a shocking truth: Grimly's species is not just any ordinary organism, but a key component in an ancient, long-forgotten technology that could revolutionize the way humanity interacts with nature. This technology, known as the \"Eco-Symbiont,\" has the potential to repair the damage inflicted on the environment by human greed and neglect.\n",
            "\n",
            "As Beaver and Arachnotron close in on Grimly's operation, they begin to realize that the CEO's true intention is not just to exploit the Eco-Symbiont for his own gain, but to control its development and use it as a tool for global domination. The duo must now decide whether to risk everything to expose Grimly's plans and protect the forest, or to keep their discovery a secret and risk losing the opportunity to change the course of human history.\n",
            "\n",
            "Rufus Grimly, sensing that Beaver and Arachnotron are closing in, becomes increasingly ruthless in his pursuit of the Eco-Symbiont. He will stop at nothing to eliminate the duo and secure his grip on the technology, even if it means sacrificing the very forest that holds the secrets to its creation.\n",
            "\n",
            "The stage is set for a high-stakes confrontation between Beaver, Arachnotron, and Rufus Grimly, as the fate of Elderwood Grove and the future of human-environmental relations hang precariously in the balance.\n",
            " **Act 3: \"The Shadow Network\"**\n",
            "\n",
            "As Beaver and Arachnotron continue to gather evidence of Grimly's plans, they stumble upon an underground network of hackers and eco-activists who have been secretly monitoring Grimly's operation. The group, known as \"The Shadow Net,\" has been working tirelessly to expose Grimly's wrongdoings and bring him to justice.\n",
            "\n",
            "Arachnotron, with their unique connection to the forest's neural networks, is able to establish a direct link with The Shadow Net, allowing them to access a vast array of classified information and resources. This newfound alliance proves to be a game-changer in their fight against Grimly, as they gain access to cutting-edge technology and expert advice from the shadowy network.\n",
            "\n",
            "Meanwhile, Beaver becomes increasingly obsessed with uncovering the truth about his grandmother's disappearance and her connection to the Eco-Symbiont. He becomes convinced that the key to unlocking the secrets of the Eco-Symbiont lies within the ancient technology that his grandmother was researching.\n",
            "\n",
            "As the stakes grow higher, Rufus Grimly becomes more desperate to eliminate Beaver and Arachnotron. He begins to use his vast resources to infiltrate The Shadow Net, sowing discord and mistrust among its members. Grimly's agents are determined to destroy any evidence of the Eco-Symbiont and silence anyone who dares to oppose him.\n",
            "\n",
            "In the midst of this chaos, Beaver and Arachnotron must navigate the treacherous landscape of the shadowy underground, all while keeping their discovery a secret from the world. They must also confront the darkness within their own pasts and the true cost of their environmental activism, as the consequences of their actions begin to take shape.\n",
            "\n",
            "The Shadow Net, sensing the gravity of the situation, calls upon its most skilled operatives to join forces with Beaver and Arachnotron. Together, they launch a daring heist to infiltrate Grimly's laboratory and steal the evidence needed to bring him down. But as they navigate the high-stakes world of corporate espionage, they realize that the true enemy may not be Rufus Grimly, but the very system that allows him to operate with impunity.\n",
            " **Act 4: \"The Heist\"**\n",
            "\n",
            "The night of the heist arrives, and Beaver, Arachnotron, and their allies from The Shadow Net infiltrate Grimly's laboratory, navigating a maze of high-security cameras, motion detectors, and genetically engineered creatures designed to protect the Eco-Symbiont. As they make their way deeper into the lab, they encounter a series of increasingly complex challenges, from decoding encrypted security systems to outsmarting Grimly's deadly traps.\n",
            "\n",
            "Arachnotron's connection to the forest's neural networks proves to be a crucial asset, as they use their symbiotic link to hack into the lab's mainframe and disable the security systems. Meanwhile, Beaver uses his engineering skills to bypass the traps and create a diversion, drawing the laboratory's guards away from the main objective.\n",
            "\n",
            "However, as they near the heart of the lab, they discover that Grimly has set a final, deadly trap in place: a genetically engineered creature designed to eliminate any threats to the Eco-Symbiont. The creature, code-named \"Erebus,\" is a behemoth of a beast, with the ability to adapt to and counter any attack.\n",
            "\n",
            "Beaver and Arachnotron must now use all their skills and resources to defeat Erebus and escape the lab with the evidence they need to bring Grimly down. But as they fight for their lives, they begin to realize that Erebus may hold the key to unlocking the secrets of the Eco-Symbiont, and that their actions may have unintended consequences for the future of human-environmental relations.\n",
            "\n",
            "As the heist reaches its climax, Beaver and Arachnotron must make a choice: to prioritize their mission and destroy Erebus, or to spare the creature and risk unleashing a powerful, uncontrollable force upon the world. The fate of Elderwood Grove, the future of human-environmental relations, and the very course of their own lives hang precariously in the balance.\n",
            " **Act 5: \"The Awakening\"**\n",
            "\n",
            "In the aftermath of the heist, Beaver and Arachnotron emerge victorious, but not without scars. They've managed to steal crucial evidence of Grimly's plans, but at a great personal cost. Erebus, the genetically engineered creature, has been defeated, but not before it reveals a shocking truth: the Eco-Symbiont is not just a technology, but a living, sentient being that has been awakened by human meddling.\n",
            "\n",
            "As the duo begins to study the evidence they've gathered, they realize that the Eco-Symbiont is not just a tool for environmental repair, but a key to unlocking a new era of human-environmental harmony. The creature's sentience and adaptability make it a game-changer in the fight against pollution and climate change.\n",
            "\n",
            "However, the news of the Eco-Symbiont's awakening spreads like wildfire, and the world is thrown into chaos. Governments, corporations, and special interest groups all vie for control of the creature, each with their own agenda for its use.\n",
            "\n",
            "Beaver and Arachnotron must now navigate this treacherous landscape, using their knowledge of the Eco-Symbiont to guide humanity towards a new era of sustainability. They must confront the darkness within themselves and the true cost of their actions, as they grapple with the responsibility of stewarding a powerful, sentient force that could change the course of human history.\n",
            "\n",
            "Rufus Grimly, still reeling from his defeat, sees an opportunity to redeem himself by offering his services to the world's governments and corporations. He proposes a partnership to harness the Eco-Symbiont's power, but at a steep price: total control over its development and deployment.\n",
            "\n",
            "As the world teeters on the brink of a new era, Beaver and Arachnotron must make a choice: to trust the governments and corporations that have failed the environment time and time again, or to forge a new path, one that prioritizes the well-being of the planet and its inhabitants above all else.\n",
            "\n",
            "In the end, it is up to Beaver and Arachnotron to awaken humanity to the true potential of the Eco-Symbiont and to guide them towards a future where technology and nature coexist in harmony. The fate of Elderwood Grove, the future of human-environmental relations, and the very course of their own lives hang precariously in the balance, as they embark on a new journey,\n",
            " **Act 5: \"The Awakening\"**\n",
            "\n",
            "In the aftermath of the heist, Beaver and Arachnotron emerge victorious, but not without scars. They've managed to steal crucial evidence of Grimly's plans, but at a great personal cost. Erebus, the genetically engineered creature, has been defeated, but not before it reveals a shocking truth: the Eco-Symbiont is not just a technology, but a living, sentient being that has been awakened by human meddling.\n",
            "\n",
            "As the duo begins to study the evidence they've gathered, they realize that the Eco-Symbiont is not just a tool for environmental repair, but a key to unlocking a new era of human-environmental harmony. The creature's sentience and adaptability make it a game-changer in the fight against pollution and climate change.\n",
            "\n",
            "However, the news of the Eco-Symbiont's awakening spreads like wildfire, and the world is thrown into chaos. Governments, corporations, and special interest groups all vie for control of the creature, each with their own agenda for its use.\n",
            "\n",
            "Beaver and Arachnotron must now navigate this treacherous landscape, using their knowledge of the Eco-Symbiont to guide humanity towards a new era of sustainability. They must confront the darkness within themselves and the true cost of their actions, as they grapple with the responsibility of stewarding a powerful, sentient force that could change the course of human history.\n",
            "\n",
            "Rufus Grimly, still reeling from his defeat, sees an opportunity to redeem himself by offering his services to the world's governments and corporations. He proposes a partnership to harness the Eco-Symbiont's power, but at a steep price: total control over its development and deployment.\n",
            "\n",
            "As the world teeters on the brink of a new era, Beaver and Arachnotron must make a choice: to trust the governments and corporations that have failed the environment time and time again, or to forge a new path, one that prioritizes the well-being of the planet and its inhabitants above all else.\n",
            "\n",
            "In the end, it is up to Beaver and Arachnotron to awaken humanity to the true potential of the Eco-Symbiont and to guide them towards a future where technology and nature coexist in harmony. The fate of Elderwood Grove, the future of human-environmental relations, and the very course of their own lives hang precariously in the balance, as they embark on a new journey to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXwdVlM6tzx6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}